{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('word2vec_embedded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>num_stop_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_punctuation_chars</th>\n",
       "      <th>word_count_after_preprocessing</th>\n",
       "      <th>num_chars_after_preprocessing</th>\n",
       "      <th>word2vec_embedded_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>111</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>82</td>\n",
       "      <td>[-0.44536713, -0.101509035, -1.050856, -0.2630...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>[-0.33632967, -0.17212276, -0.87967306, -0.000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>110</td>\n",
       "      <td>[-0.23400958, -0.0057888003, -0.75428146, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>[-0.3609581, -0.032422144, -1.5170497, -0.1118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>[-0.10801181, -0.021496635, -1.3440485, -0.605...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  word_count  num_stop_words  num_chars  num_punctuation_chars  \\\n",
       "0   ham          20               4        111                      9   \n",
       "1   ham           6               0         29                      6   \n",
       "2  spam          28               5        155                      5   \n",
       "3   ham          11               2         49                      6   \n",
       "4   ham          13               6         61                      2   \n",
       "\n",
       "   word_count_after_preprocessing  num_chars_after_preprocessing  \\\n",
       "0                              16                             82   \n",
       "1                               6                             23   \n",
       "2                              20                            110   \n",
       "3                               9                             35   \n",
       "4                               8                             40   \n",
       "\n",
       "                            word2vec_embedded_vector  \n",
       "0  [-0.44536713, -0.101509035, -1.050856, -0.2630...  \n",
       "1  [-0.33632967, -0.17212276, -0.87967306, -0.000...  \n",
       "2  [-0.23400958, -0.0057888003, -0.75428146, 0.01...  \n",
       "3  [-0.3609581, -0.032422144, -1.5170497, -0.1118...  \n",
       "4  [-0.10801181, -0.021496635, -1.3440485, -0.605...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the embedded_vectors are stored as a list in a single column we have to store them in different column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[-0.44536713, -0.101509035, -1.050856, -0.26304704, 0.17670652, -0.38556227, -0.26386967, 0.7860364, 0.05022185, -0.44155043, -0.07436119, 0.30960253, -0.45769256, -0.39246804, -0.00664151, 0.23553985, 0.12444629, -0.44299844, -0.075095676, -0.40202767, -0.07108323, 0.07944961, -0.26789296, -0.11591704, 0.020144098, -0.31259376, -0.5498882, -0.1030678, -0.5509271, -0.2152968, 0.41478682, -0.057546005, -0.063271694, 0.39572355, -0.60937107, -0.60341763, -0.25892988, 0.40874475, -0.14386727, -0.21467493, 0.7708749, 0.029603157, -0.2517229, -0.077580765, -0.51987696, -0.6890223, -0.0099074375, 0.5231168, 0.6201726, 0.52448237, 0.27219087, -0.3323066, 0.105960965, -0.23726727, 0.45822117, 0.35581344, 0.1408332, -0.0224453, -0.4689358, 0.29117593, 0.33417055, 0.23974164, -0.15251426, 0.35562056, -0.7185296, -0.18573803, -0.11705361, -0.11077832, -0.05695879, -0.024721935, -0.33903083, -0.59627044, 0.015256643, -0.41224808, 0.26141384, -0.0600885, 0.3465434, 0.38989437, -0.0038757946, 0.3037718, 0.09298281, -0.053571217, -0.41314134, 1.2156194, -0.31297424, -0.19659618, 0.19133262, 0.8249279, 0.67775774, 1.0330545, 0.3637597, 0.42347145, -0.1831246, 0.123134404, 0.4214869, -0.08538388, 0.096359864, -0.31131303, 0.5900591, -0.06811023]'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['word2vec_embedded_vector'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# Function to convert string to list\n",
    "def convert_to_list(vector_str):\n",
    "    return ast.literal_eval(vector_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the conversion to the 'word2vec_embedded_vector'\n",
    "df['word2vec_embedded_vector'] = df['word2vec_embedded_vector'].apply(convert_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>num_stop_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_punctuation_chars</th>\n",
       "      <th>word_count_after_preprocessing</th>\n",
       "      <th>num_chars_after_preprocessing</th>\n",
       "      <th>word2vec_embedded_vector</th>\n",
       "      <th>vector_dim_1</th>\n",
       "      <th>vector_dim_2</th>\n",
       "      <th>...</th>\n",
       "      <th>vector_dim_91</th>\n",
       "      <th>vector_dim_92</th>\n",
       "      <th>vector_dim_93</th>\n",
       "      <th>vector_dim_94</th>\n",
       "      <th>vector_dim_95</th>\n",
       "      <th>vector_dim_96</th>\n",
       "      <th>vector_dim_97</th>\n",
       "      <th>vector_dim_98</th>\n",
       "      <th>vector_dim_99</th>\n",
       "      <th>vector_dim_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>111</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>82</td>\n",
       "      <td>[-0.44536713, -0.101509035, -1.050856, -0.2630...</td>\n",
       "      <td>-0.445367</td>\n",
       "      <td>-0.101509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363760</td>\n",
       "      <td>0.423471</td>\n",
       "      <td>-0.183125</td>\n",
       "      <td>0.123134</td>\n",
       "      <td>0.421487</td>\n",
       "      <td>-0.085384</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>-0.311313</td>\n",
       "      <td>0.590059</td>\n",
       "      <td>-0.068110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>[-0.33632967, -0.17212276, -0.87967306, -0.000...</td>\n",
       "      <td>-0.336330</td>\n",
       "      <td>-0.172123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178105</td>\n",
       "      <td>0.469185</td>\n",
       "      <td>-0.159354</td>\n",
       "      <td>0.181287</td>\n",
       "      <td>0.370253</td>\n",
       "      <td>-0.151212</td>\n",
       "      <td>-0.029242</td>\n",
       "      <td>0.017299</td>\n",
       "      <td>0.309134</td>\n",
       "      <td>-0.183176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>110</td>\n",
       "      <td>[-0.23400958, -0.0057888003, -0.75428146, 0.01...</td>\n",
       "      <td>-0.234010</td>\n",
       "      <td>-0.005789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179313</td>\n",
       "      <td>0.245597</td>\n",
       "      <td>-0.074740</td>\n",
       "      <td>0.326302</td>\n",
       "      <td>0.246354</td>\n",
       "      <td>0.009656</td>\n",
       "      <td>-0.045136</td>\n",
       "      <td>-0.300550</td>\n",
       "      <td>0.424266</td>\n",
       "      <td>-0.487235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>[-0.3609581, -0.032422144, -1.5170497, -0.1118...</td>\n",
       "      <td>-0.360958</td>\n",
       "      <td>-0.032422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446681</td>\n",
       "      <td>0.821034</td>\n",
       "      <td>-0.187948</td>\n",
       "      <td>0.338291</td>\n",
       "      <td>0.571384</td>\n",
       "      <td>-0.449642</td>\n",
       "      <td>-0.185700</td>\n",
       "      <td>0.029383</td>\n",
       "      <td>0.535742</td>\n",
       "      <td>-0.069742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>[-0.10801181, -0.021496635, -1.3440485, -0.605...</td>\n",
       "      <td>-0.108012</td>\n",
       "      <td>-0.021497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748294</td>\n",
       "      <td>-0.040061</td>\n",
       "      <td>-0.212751</td>\n",
       "      <td>0.148267</td>\n",
       "      <td>-0.139923</td>\n",
       "      <td>-0.006358</td>\n",
       "      <td>0.034125</td>\n",
       "      <td>-0.220026</td>\n",
       "      <td>1.054043</td>\n",
       "      <td>-0.075081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  word_count  num_stop_words  num_chars  num_punctuation_chars  \\\n",
       "0   ham          20               4        111                      9   \n",
       "1   ham           6               0         29                      6   \n",
       "2  spam          28               5        155                      5   \n",
       "3   ham          11               2         49                      6   \n",
       "4   ham          13               6         61                      2   \n",
       "\n",
       "   word_count_after_preprocessing  num_chars_after_preprocessing  \\\n",
       "0                              16                             82   \n",
       "1                               6                             23   \n",
       "2                              20                            110   \n",
       "3                               9                             35   \n",
       "4                               8                             40   \n",
       "\n",
       "                            word2vec_embedded_vector  vector_dim_1  \\\n",
       "0  [-0.44536713, -0.101509035, -1.050856, -0.2630...     -0.445367   \n",
       "1  [-0.33632967, -0.17212276, -0.87967306, -0.000...     -0.336330   \n",
       "2  [-0.23400958, -0.0057888003, -0.75428146, 0.01...     -0.234010   \n",
       "3  [-0.3609581, -0.032422144, -1.5170497, -0.1118...     -0.360958   \n",
       "4  [-0.10801181, -0.021496635, -1.3440485, -0.605...     -0.108012   \n",
       "\n",
       "   vector_dim_2  ...  vector_dim_91  vector_dim_92  vector_dim_93  \\\n",
       "0     -0.101509  ...       0.363760       0.423471      -0.183125   \n",
       "1     -0.172123  ...       0.178105       0.469185      -0.159354   \n",
       "2     -0.005789  ...       0.179313       0.245597      -0.074740   \n",
       "3     -0.032422  ...       0.446681       0.821034      -0.187948   \n",
       "4     -0.021497  ...       0.748294      -0.040061      -0.212751   \n",
       "\n",
       "   vector_dim_94  vector_dim_95  vector_dim_96  vector_dim_97  vector_dim_98  \\\n",
       "0       0.123134       0.421487      -0.085384       0.096360      -0.311313   \n",
       "1       0.181287       0.370253      -0.151212      -0.029242       0.017299   \n",
       "2       0.326302       0.246354       0.009656      -0.045136      -0.300550   \n",
       "3       0.338291       0.571384      -0.449642      -0.185700       0.029383   \n",
       "4       0.148267      -0.139923      -0.006358       0.034125      -0.220026   \n",
       "\n",
       "   vector_dim_99  vector_dim_100  \n",
       "0       0.590059       -0.068110  \n",
       "1       0.309134       -0.183176  \n",
       "2       0.424266       -0.487235  \n",
       "3       0.535742       -0.069742  \n",
       "4       1.054043       -0.075081  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from the list of vectors\n",
    "expanded_df = df['word2vec_embedded_vector'].apply(pd.Series)\n",
    "\n",
    "# Optionally, rename the columns\n",
    "expanded_df.columns = [f'vector_dim_{i+1}' for i in range(expanded_df.shape[1])]\n",
    "\n",
    "# Concatenate the new DataFrame with the original DataFrame (if needed)\n",
    "final_df = pd.concat([df, expanded_df], axis=1)\n",
    "\n",
    "# Display the final DataFrame\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop(columns='word2vec_embedded_vector',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>num_stop_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_punctuation_chars</th>\n",
       "      <th>word_count_after_preprocessing</th>\n",
       "      <th>num_chars_after_preprocessing</th>\n",
       "      <th>vector_dim_1</th>\n",
       "      <th>vector_dim_2</th>\n",
       "      <th>vector_dim_3</th>\n",
       "      <th>...</th>\n",
       "      <th>vector_dim_91</th>\n",
       "      <th>vector_dim_92</th>\n",
       "      <th>vector_dim_93</th>\n",
       "      <th>vector_dim_94</th>\n",
       "      <th>vector_dim_95</th>\n",
       "      <th>vector_dim_96</th>\n",
       "      <th>vector_dim_97</th>\n",
       "      <th>vector_dim_98</th>\n",
       "      <th>vector_dim_99</th>\n",
       "      <th>vector_dim_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>111</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>82</td>\n",
       "      <td>-0.445367</td>\n",
       "      <td>-0.101509</td>\n",
       "      <td>-1.050856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363760</td>\n",
       "      <td>0.423471</td>\n",
       "      <td>-0.183125</td>\n",
       "      <td>0.123134</td>\n",
       "      <td>0.421487</td>\n",
       "      <td>-0.085384</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>-0.311313</td>\n",
       "      <td>0.590059</td>\n",
       "      <td>-0.068110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.336330</td>\n",
       "      <td>-0.172123</td>\n",
       "      <td>-0.879673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178105</td>\n",
       "      <td>0.469185</td>\n",
       "      <td>-0.159354</td>\n",
       "      <td>0.181287</td>\n",
       "      <td>0.370253</td>\n",
       "      <td>-0.151212</td>\n",
       "      <td>-0.029242</td>\n",
       "      <td>0.017299</td>\n",
       "      <td>0.309134</td>\n",
       "      <td>-0.183176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>110</td>\n",
       "      <td>-0.234010</td>\n",
       "      <td>-0.005789</td>\n",
       "      <td>-0.754281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179313</td>\n",
       "      <td>0.245597</td>\n",
       "      <td>-0.074740</td>\n",
       "      <td>0.326302</td>\n",
       "      <td>0.246354</td>\n",
       "      <td>0.009656</td>\n",
       "      <td>-0.045136</td>\n",
       "      <td>-0.300550</td>\n",
       "      <td>0.424266</td>\n",
       "      <td>-0.487235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.360958</td>\n",
       "      <td>-0.032422</td>\n",
       "      <td>-1.517050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446681</td>\n",
       "      <td>0.821034</td>\n",
       "      <td>-0.187948</td>\n",
       "      <td>0.338291</td>\n",
       "      <td>0.571384</td>\n",
       "      <td>-0.449642</td>\n",
       "      <td>-0.185700</td>\n",
       "      <td>0.029383</td>\n",
       "      <td>0.535742</td>\n",
       "      <td>-0.069742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>-0.108012</td>\n",
       "      <td>-0.021497</td>\n",
       "      <td>-1.344048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748294</td>\n",
       "      <td>-0.040061</td>\n",
       "      <td>-0.212751</td>\n",
       "      <td>0.148267</td>\n",
       "      <td>-0.139923</td>\n",
       "      <td>-0.006358</td>\n",
       "      <td>0.034125</td>\n",
       "      <td>-0.220026</td>\n",
       "      <td>1.054043</td>\n",
       "      <td>-0.075081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14612</th>\n",
       "      <td>ham</td>\n",
       "      <td>131</td>\n",
       "      <td>59</td>\n",
       "      <td>745</td>\n",
       "      <td>24</td>\n",
       "      <td>74</td>\n",
       "      <td>517</td>\n",
       "      <td>-0.251930</td>\n",
       "      <td>-0.063167</td>\n",
       "      <td>-1.194805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651287</td>\n",
       "      <td>-0.020689</td>\n",
       "      <td>0.026659</td>\n",
       "      <td>0.290871</td>\n",
       "      <td>0.011417</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>-0.094306</td>\n",
       "      <td>-0.413245</td>\n",
       "      <td>0.968019</td>\n",
       "      <td>-0.213552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14613</th>\n",
       "      <td>ham</td>\n",
       "      <td>121</td>\n",
       "      <td>62</td>\n",
       "      <td>670</td>\n",
       "      <td>18</td>\n",
       "      <td>63</td>\n",
       "      <td>430</td>\n",
       "      <td>-0.188276</td>\n",
       "      <td>-0.060201</td>\n",
       "      <td>-1.234956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.824768</td>\n",
       "      <td>-0.053670</td>\n",
       "      <td>0.166092</td>\n",
       "      <td>0.286987</td>\n",
       "      <td>-0.115114</td>\n",
       "      <td>-0.072424</td>\n",
       "      <td>-0.317539</td>\n",
       "      <td>-0.333784</td>\n",
       "      <td>0.966461</td>\n",
       "      <td>-0.284511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14614</th>\n",
       "      <td>ham</td>\n",
       "      <td>141</td>\n",
       "      <td>72</td>\n",
       "      <td>770</td>\n",
       "      <td>9</td>\n",
       "      <td>69</td>\n",
       "      <td>490</td>\n",
       "      <td>-0.237662</td>\n",
       "      <td>-0.076631</td>\n",
       "      <td>-1.247667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.801583</td>\n",
       "      <td>-0.036325</td>\n",
       "      <td>0.300470</td>\n",
       "      <td>0.293482</td>\n",
       "      <td>-0.156067</td>\n",
       "      <td>-0.117382</td>\n",
       "      <td>-0.343503</td>\n",
       "      <td>-0.324489</td>\n",
       "      <td>0.929325</td>\n",
       "      <td>-0.190733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14615</th>\n",
       "      <td>ham</td>\n",
       "      <td>86</td>\n",
       "      <td>47</td>\n",
       "      <td>448</td>\n",
       "      <td>16</td>\n",
       "      <td>41</td>\n",
       "      <td>266</td>\n",
       "      <td>-0.340769</td>\n",
       "      <td>-0.058809</td>\n",
       "      <td>-1.138488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.681083</td>\n",
       "      <td>0.017155</td>\n",
       "      <td>0.198969</td>\n",
       "      <td>0.235866</td>\n",
       "      <td>-0.116819</td>\n",
       "      <td>-0.105489</td>\n",
       "      <td>-0.097344</td>\n",
       "      <td>-0.315073</td>\n",
       "      <td>0.851146</td>\n",
       "      <td>-0.265891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14616</th>\n",
       "      <td>ham</td>\n",
       "      <td>260</td>\n",
       "      <td>139</td>\n",
       "      <td>1363</td>\n",
       "      <td>35</td>\n",
       "      <td>126</td>\n",
       "      <td>845</td>\n",
       "      <td>-0.298359</td>\n",
       "      <td>-0.085764</td>\n",
       "      <td>-1.240260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803427</td>\n",
       "      <td>0.030206</td>\n",
       "      <td>0.115804</td>\n",
       "      <td>0.236635</td>\n",
       "      <td>-0.126242</td>\n",
       "      <td>-0.068015</td>\n",
       "      <td>-0.212386</td>\n",
       "      <td>-0.343189</td>\n",
       "      <td>0.971015</td>\n",
       "      <td>-0.242007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14617 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  word_count  num_stop_words  num_chars  num_punctuation_chars  \\\n",
       "0       ham          20               4        111                      9   \n",
       "1       ham           6               0         29                      6   \n",
       "2      spam          28               5        155                      5   \n",
       "3       ham          11               2         49                      6   \n",
       "4       ham          13               6         61                      2   \n",
       "...     ...         ...             ...        ...                    ...   \n",
       "14612   ham         131              59        745                     24   \n",
       "14613   ham         121              62        670                     18   \n",
       "14614   ham         141              72        770                      9   \n",
       "14615   ham          86              47        448                     16   \n",
       "14616   ham         260             139       1363                     35   \n",
       "\n",
       "       word_count_after_preprocessing  num_chars_after_preprocessing  \\\n",
       "0                                  16                             82   \n",
       "1                                   6                             23   \n",
       "2                                  20                            110   \n",
       "3                                   9                             35   \n",
       "4                                   8                             40   \n",
       "...                               ...                            ...   \n",
       "14612                              74                            517   \n",
       "14613                              63                            430   \n",
       "14614                              69                            490   \n",
       "14615                              41                            266   \n",
       "14616                             126                            845   \n",
       "\n",
       "       vector_dim_1  vector_dim_2  vector_dim_3  ...  vector_dim_91  \\\n",
       "0         -0.445367     -0.101509     -1.050856  ...       0.363760   \n",
       "1         -0.336330     -0.172123     -0.879673  ...       0.178105   \n",
       "2         -0.234010     -0.005789     -0.754281  ...       0.179313   \n",
       "3         -0.360958     -0.032422     -1.517050  ...       0.446681   \n",
       "4         -0.108012     -0.021497     -1.344048  ...       0.748294   \n",
       "...             ...           ...           ...  ...            ...   \n",
       "14612     -0.251930     -0.063167     -1.194805  ...       0.651287   \n",
       "14613     -0.188276     -0.060201     -1.234956  ...       0.824768   \n",
       "14614     -0.237662     -0.076631     -1.247667  ...       0.801583   \n",
       "14615     -0.340769     -0.058809     -1.138488  ...       0.681083   \n",
       "14616     -0.298359     -0.085764     -1.240260  ...       0.803427   \n",
       "\n",
       "       vector_dim_92  vector_dim_93  vector_dim_94  vector_dim_95  \\\n",
       "0           0.423471      -0.183125       0.123134       0.421487   \n",
       "1           0.469185      -0.159354       0.181287       0.370253   \n",
       "2           0.245597      -0.074740       0.326302       0.246354   \n",
       "3           0.821034      -0.187948       0.338291       0.571384   \n",
       "4          -0.040061      -0.212751       0.148267      -0.139923   \n",
       "...              ...            ...            ...            ...   \n",
       "14612      -0.020689       0.026659       0.290871       0.011417   \n",
       "14613      -0.053670       0.166092       0.286987      -0.115114   \n",
       "14614      -0.036325       0.300470       0.293482      -0.156067   \n",
       "14615       0.017155       0.198969       0.235866      -0.116819   \n",
       "14616       0.030206       0.115804       0.236635      -0.126242   \n",
       "\n",
       "       vector_dim_96  vector_dim_97  vector_dim_98  vector_dim_99  \\\n",
       "0          -0.085384       0.096360      -0.311313       0.590059   \n",
       "1          -0.151212      -0.029242       0.017299       0.309134   \n",
       "2           0.009656      -0.045136      -0.300550       0.424266   \n",
       "3          -0.449642      -0.185700       0.029383       0.535742   \n",
       "4          -0.006358       0.034125      -0.220026       1.054043   \n",
       "...              ...            ...            ...            ...   \n",
       "14612       0.002676      -0.094306      -0.413245       0.968019   \n",
       "14613      -0.072424      -0.317539      -0.333784       0.966461   \n",
       "14614      -0.117382      -0.343503      -0.324489       0.929325   \n",
       "14615      -0.105489      -0.097344      -0.315073       0.851146   \n",
       "14616      -0.068015      -0.212386      -0.343189       0.971015   \n",
       "\n",
       "       vector_dim_100  \n",
       "0           -0.068110  \n",
       "1           -0.183176  \n",
       "2           -0.487235  \n",
       "3           -0.069742  \n",
       "4           -0.075081  \n",
       "...               ...  \n",
       "14612       -0.213552  \n",
       "14613       -0.284511  \n",
       "14614       -0.190733  \n",
       "14615       -0.265891  \n",
       "14616       -0.242007  \n",
       "\n",
       "[14617 rows x 107 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('training_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "final_df['label'] = label_encoder.fit_transform(final_df['label'])\n",
    "#0 for ham 1 for spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>num_stop_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_punctuation_chars</th>\n",
       "      <th>word_count_after_preprocessing</th>\n",
       "      <th>num_chars_after_preprocessing</th>\n",
       "      <th>vector_dim_1</th>\n",
       "      <th>vector_dim_2</th>\n",
       "      <th>vector_dim_3</th>\n",
       "      <th>...</th>\n",
       "      <th>vector_dim_91</th>\n",
       "      <th>vector_dim_92</th>\n",
       "      <th>vector_dim_93</th>\n",
       "      <th>vector_dim_94</th>\n",
       "      <th>vector_dim_95</th>\n",
       "      <th>vector_dim_96</th>\n",
       "      <th>vector_dim_97</th>\n",
       "      <th>vector_dim_98</th>\n",
       "      <th>vector_dim_99</th>\n",
       "      <th>vector_dim_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>111</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>82</td>\n",
       "      <td>-0.445367</td>\n",
       "      <td>-0.101509</td>\n",
       "      <td>-1.050856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363760</td>\n",
       "      <td>0.423471</td>\n",
       "      <td>-0.183125</td>\n",
       "      <td>0.123134</td>\n",
       "      <td>0.421487</td>\n",
       "      <td>-0.085384</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>-0.311313</td>\n",
       "      <td>0.590059</td>\n",
       "      <td>-0.068110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.336330</td>\n",
       "      <td>-0.172123</td>\n",
       "      <td>-0.879673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178105</td>\n",
       "      <td>0.469185</td>\n",
       "      <td>-0.159354</td>\n",
       "      <td>0.181287</td>\n",
       "      <td>0.370253</td>\n",
       "      <td>-0.151212</td>\n",
       "      <td>-0.029242</td>\n",
       "      <td>0.017299</td>\n",
       "      <td>0.309134</td>\n",
       "      <td>-0.183176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>110</td>\n",
       "      <td>-0.234010</td>\n",
       "      <td>-0.005789</td>\n",
       "      <td>-0.754281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179313</td>\n",
       "      <td>0.245597</td>\n",
       "      <td>-0.074740</td>\n",
       "      <td>0.326302</td>\n",
       "      <td>0.246354</td>\n",
       "      <td>0.009656</td>\n",
       "      <td>-0.045136</td>\n",
       "      <td>-0.300550</td>\n",
       "      <td>0.424266</td>\n",
       "      <td>-0.487235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.360958</td>\n",
       "      <td>-0.032422</td>\n",
       "      <td>-1.517050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446681</td>\n",
       "      <td>0.821034</td>\n",
       "      <td>-0.187948</td>\n",
       "      <td>0.338291</td>\n",
       "      <td>0.571384</td>\n",
       "      <td>-0.449642</td>\n",
       "      <td>-0.185700</td>\n",
       "      <td>0.029383</td>\n",
       "      <td>0.535742</td>\n",
       "      <td>-0.069742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>-0.108012</td>\n",
       "      <td>-0.021497</td>\n",
       "      <td>-1.344048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748294</td>\n",
       "      <td>-0.040061</td>\n",
       "      <td>-0.212751</td>\n",
       "      <td>0.148267</td>\n",
       "      <td>-0.139923</td>\n",
       "      <td>-0.006358</td>\n",
       "      <td>0.034125</td>\n",
       "      <td>-0.220026</td>\n",
       "      <td>1.054043</td>\n",
       "      <td>-0.075081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14612</th>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "      <td>59</td>\n",
       "      <td>745</td>\n",
       "      <td>24</td>\n",
       "      <td>74</td>\n",
       "      <td>517</td>\n",
       "      <td>-0.251930</td>\n",
       "      <td>-0.063167</td>\n",
       "      <td>-1.194805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651287</td>\n",
       "      <td>-0.020689</td>\n",
       "      <td>0.026659</td>\n",
       "      <td>0.290871</td>\n",
       "      <td>0.011417</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>-0.094306</td>\n",
       "      <td>-0.413245</td>\n",
       "      <td>0.968019</td>\n",
       "      <td>-0.213552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14613</th>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>62</td>\n",
       "      <td>670</td>\n",
       "      <td>18</td>\n",
       "      <td>63</td>\n",
       "      <td>430</td>\n",
       "      <td>-0.188276</td>\n",
       "      <td>-0.060201</td>\n",
       "      <td>-1.234956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.824768</td>\n",
       "      <td>-0.053670</td>\n",
       "      <td>0.166092</td>\n",
       "      <td>0.286987</td>\n",
       "      <td>-0.115114</td>\n",
       "      <td>-0.072424</td>\n",
       "      <td>-0.317539</td>\n",
       "      <td>-0.333784</td>\n",
       "      <td>0.966461</td>\n",
       "      <td>-0.284511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14614</th>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>72</td>\n",
       "      <td>770</td>\n",
       "      <td>9</td>\n",
       "      <td>69</td>\n",
       "      <td>490</td>\n",
       "      <td>-0.237662</td>\n",
       "      <td>-0.076631</td>\n",
       "      <td>-1.247667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.801583</td>\n",
       "      <td>-0.036325</td>\n",
       "      <td>0.300470</td>\n",
       "      <td>0.293482</td>\n",
       "      <td>-0.156067</td>\n",
       "      <td>-0.117382</td>\n",
       "      <td>-0.343503</td>\n",
       "      <td>-0.324489</td>\n",
       "      <td>0.929325</td>\n",
       "      <td>-0.190733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14615</th>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>47</td>\n",
       "      <td>448</td>\n",
       "      <td>16</td>\n",
       "      <td>41</td>\n",
       "      <td>266</td>\n",
       "      <td>-0.340769</td>\n",
       "      <td>-0.058809</td>\n",
       "      <td>-1.138488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.681083</td>\n",
       "      <td>0.017155</td>\n",
       "      <td>0.198969</td>\n",
       "      <td>0.235866</td>\n",
       "      <td>-0.116819</td>\n",
       "      <td>-0.105489</td>\n",
       "      <td>-0.097344</td>\n",
       "      <td>-0.315073</td>\n",
       "      <td>0.851146</td>\n",
       "      <td>-0.265891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14616</th>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "      <td>139</td>\n",
       "      <td>1363</td>\n",
       "      <td>35</td>\n",
       "      <td>126</td>\n",
       "      <td>845</td>\n",
       "      <td>-0.298359</td>\n",
       "      <td>-0.085764</td>\n",
       "      <td>-1.240260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803427</td>\n",
       "      <td>0.030206</td>\n",
       "      <td>0.115804</td>\n",
       "      <td>0.236635</td>\n",
       "      <td>-0.126242</td>\n",
       "      <td>-0.068015</td>\n",
       "      <td>-0.212386</td>\n",
       "      <td>-0.343189</td>\n",
       "      <td>0.971015</td>\n",
       "      <td>-0.242007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14617 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  word_count  num_stop_words  num_chars  num_punctuation_chars  \\\n",
       "0          0          20               4        111                      9   \n",
       "1          0           6               0         29                      6   \n",
       "2          1          28               5        155                      5   \n",
       "3          0          11               2         49                      6   \n",
       "4          0          13               6         61                      2   \n",
       "...      ...         ...             ...        ...                    ...   \n",
       "14612      0         131              59        745                     24   \n",
       "14613      0         121              62        670                     18   \n",
       "14614      0         141              72        770                      9   \n",
       "14615      0          86              47        448                     16   \n",
       "14616      0         260             139       1363                     35   \n",
       "\n",
       "       word_count_after_preprocessing  num_chars_after_preprocessing  \\\n",
       "0                                  16                             82   \n",
       "1                                   6                             23   \n",
       "2                                  20                            110   \n",
       "3                                   9                             35   \n",
       "4                                   8                             40   \n",
       "...                               ...                            ...   \n",
       "14612                              74                            517   \n",
       "14613                              63                            430   \n",
       "14614                              69                            490   \n",
       "14615                              41                            266   \n",
       "14616                             126                            845   \n",
       "\n",
       "       vector_dim_1  vector_dim_2  vector_dim_3  ...  vector_dim_91  \\\n",
       "0         -0.445367     -0.101509     -1.050856  ...       0.363760   \n",
       "1         -0.336330     -0.172123     -0.879673  ...       0.178105   \n",
       "2         -0.234010     -0.005789     -0.754281  ...       0.179313   \n",
       "3         -0.360958     -0.032422     -1.517050  ...       0.446681   \n",
       "4         -0.108012     -0.021497     -1.344048  ...       0.748294   \n",
       "...             ...           ...           ...  ...            ...   \n",
       "14612     -0.251930     -0.063167     -1.194805  ...       0.651287   \n",
       "14613     -0.188276     -0.060201     -1.234956  ...       0.824768   \n",
       "14614     -0.237662     -0.076631     -1.247667  ...       0.801583   \n",
       "14615     -0.340769     -0.058809     -1.138488  ...       0.681083   \n",
       "14616     -0.298359     -0.085764     -1.240260  ...       0.803427   \n",
       "\n",
       "       vector_dim_92  vector_dim_93  vector_dim_94  vector_dim_95  \\\n",
       "0           0.423471      -0.183125       0.123134       0.421487   \n",
       "1           0.469185      -0.159354       0.181287       0.370253   \n",
       "2           0.245597      -0.074740       0.326302       0.246354   \n",
       "3           0.821034      -0.187948       0.338291       0.571384   \n",
       "4          -0.040061      -0.212751       0.148267      -0.139923   \n",
       "...              ...            ...            ...            ...   \n",
       "14612      -0.020689       0.026659       0.290871       0.011417   \n",
       "14613      -0.053670       0.166092       0.286987      -0.115114   \n",
       "14614      -0.036325       0.300470       0.293482      -0.156067   \n",
       "14615       0.017155       0.198969       0.235866      -0.116819   \n",
       "14616       0.030206       0.115804       0.236635      -0.126242   \n",
       "\n",
       "       vector_dim_96  vector_dim_97  vector_dim_98  vector_dim_99  \\\n",
       "0          -0.085384       0.096360      -0.311313       0.590059   \n",
       "1          -0.151212      -0.029242       0.017299       0.309134   \n",
       "2           0.009656      -0.045136      -0.300550       0.424266   \n",
       "3          -0.449642      -0.185700       0.029383       0.535742   \n",
       "4          -0.006358       0.034125      -0.220026       1.054043   \n",
       "...              ...            ...            ...            ...   \n",
       "14612       0.002676      -0.094306      -0.413245       0.968019   \n",
       "14613      -0.072424      -0.317539      -0.333784       0.966461   \n",
       "14614      -0.117382      -0.343503      -0.324489       0.929325   \n",
       "14615      -0.105489      -0.097344      -0.315073       0.851146   \n",
       "14616      -0.068015      -0.212386      -0.343189       0.971015   \n",
       "\n",
       "       vector_dim_100  \n",
       "0           -0.068110  \n",
       "1           -0.183176  \n",
       "2           -0.487235  \n",
       "3           -0.069742  \n",
       "4           -0.075081  \n",
       "...               ...  \n",
       "14612       -0.213552  \n",
       "14613       -0.284511  \n",
       "14614       -0.190733  \n",
       "14615       -0.265891  \n",
       "14616       -0.242007  \n",
       "\n",
       "[14617 rows x 107 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=final_df.drop(columns='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>num_stop_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_punctuation_chars</th>\n",
       "      <th>word_count_after_preprocessing</th>\n",
       "      <th>num_chars_after_preprocessing</th>\n",
       "      <th>vector_dim_1</th>\n",
       "      <th>vector_dim_2</th>\n",
       "      <th>vector_dim_3</th>\n",
       "      <th>vector_dim_4</th>\n",
       "      <th>...</th>\n",
       "      <th>vector_dim_91</th>\n",
       "      <th>vector_dim_92</th>\n",
       "      <th>vector_dim_93</th>\n",
       "      <th>vector_dim_94</th>\n",
       "      <th>vector_dim_95</th>\n",
       "      <th>vector_dim_96</th>\n",
       "      <th>vector_dim_97</th>\n",
       "      <th>vector_dim_98</th>\n",
       "      <th>vector_dim_99</th>\n",
       "      <th>vector_dim_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>111</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>82</td>\n",
       "      <td>-0.445367</td>\n",
       "      <td>-0.101509</td>\n",
       "      <td>-1.050856</td>\n",
       "      <td>-0.263047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363760</td>\n",
       "      <td>0.423471</td>\n",
       "      <td>-0.183125</td>\n",
       "      <td>0.123134</td>\n",
       "      <td>0.421487</td>\n",
       "      <td>-0.085384</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>-0.311313</td>\n",
       "      <td>0.590059</td>\n",
       "      <td>-0.068110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.336330</td>\n",
       "      <td>-0.172123</td>\n",
       "      <td>-0.879673</td>\n",
       "      <td>-0.000336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178105</td>\n",
       "      <td>0.469185</td>\n",
       "      <td>-0.159354</td>\n",
       "      <td>0.181287</td>\n",
       "      <td>0.370253</td>\n",
       "      <td>-0.151212</td>\n",
       "      <td>-0.029242</td>\n",
       "      <td>0.017299</td>\n",
       "      <td>0.309134</td>\n",
       "      <td>-0.183176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>110</td>\n",
       "      <td>-0.234010</td>\n",
       "      <td>-0.005789</td>\n",
       "      <td>-0.754281</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179313</td>\n",
       "      <td>0.245597</td>\n",
       "      <td>-0.074740</td>\n",
       "      <td>0.326302</td>\n",
       "      <td>0.246354</td>\n",
       "      <td>0.009656</td>\n",
       "      <td>-0.045136</td>\n",
       "      <td>-0.300550</td>\n",
       "      <td>0.424266</td>\n",
       "      <td>-0.487235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.360958</td>\n",
       "      <td>-0.032422</td>\n",
       "      <td>-1.517050</td>\n",
       "      <td>-0.111853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446681</td>\n",
       "      <td>0.821034</td>\n",
       "      <td>-0.187948</td>\n",
       "      <td>0.338291</td>\n",
       "      <td>0.571384</td>\n",
       "      <td>-0.449642</td>\n",
       "      <td>-0.185700</td>\n",
       "      <td>0.029383</td>\n",
       "      <td>0.535742</td>\n",
       "      <td>-0.069742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>-0.108012</td>\n",
       "      <td>-0.021497</td>\n",
       "      <td>-1.344048</td>\n",
       "      <td>-0.605199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748294</td>\n",
       "      <td>-0.040061</td>\n",
       "      <td>-0.212751</td>\n",
       "      <td>0.148267</td>\n",
       "      <td>-0.139923</td>\n",
       "      <td>-0.006358</td>\n",
       "      <td>0.034125</td>\n",
       "      <td>-0.220026</td>\n",
       "      <td>1.054043</td>\n",
       "      <td>-0.075081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14612</th>\n",
       "      <td>131</td>\n",
       "      <td>59</td>\n",
       "      <td>745</td>\n",
       "      <td>24</td>\n",
       "      <td>74</td>\n",
       "      <td>517</td>\n",
       "      <td>-0.251930</td>\n",
       "      <td>-0.063167</td>\n",
       "      <td>-1.194805</td>\n",
       "      <td>-0.387192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651287</td>\n",
       "      <td>-0.020689</td>\n",
       "      <td>0.026659</td>\n",
       "      <td>0.290871</td>\n",
       "      <td>0.011417</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>-0.094306</td>\n",
       "      <td>-0.413245</td>\n",
       "      <td>0.968019</td>\n",
       "      <td>-0.213552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14613</th>\n",
       "      <td>121</td>\n",
       "      <td>62</td>\n",
       "      <td>670</td>\n",
       "      <td>18</td>\n",
       "      <td>63</td>\n",
       "      <td>430</td>\n",
       "      <td>-0.188276</td>\n",
       "      <td>-0.060201</td>\n",
       "      <td>-1.234956</td>\n",
       "      <td>-0.394533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.824768</td>\n",
       "      <td>-0.053670</td>\n",
       "      <td>0.166092</td>\n",
       "      <td>0.286987</td>\n",
       "      <td>-0.115114</td>\n",
       "      <td>-0.072424</td>\n",
       "      <td>-0.317539</td>\n",
       "      <td>-0.333784</td>\n",
       "      <td>0.966461</td>\n",
       "      <td>-0.284511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14614</th>\n",
       "      <td>141</td>\n",
       "      <td>72</td>\n",
       "      <td>770</td>\n",
       "      <td>9</td>\n",
       "      <td>69</td>\n",
       "      <td>490</td>\n",
       "      <td>-0.237662</td>\n",
       "      <td>-0.076631</td>\n",
       "      <td>-1.247667</td>\n",
       "      <td>-0.374296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.801583</td>\n",
       "      <td>-0.036325</td>\n",
       "      <td>0.300470</td>\n",
       "      <td>0.293482</td>\n",
       "      <td>-0.156067</td>\n",
       "      <td>-0.117382</td>\n",
       "      <td>-0.343503</td>\n",
       "      <td>-0.324489</td>\n",
       "      <td>0.929325</td>\n",
       "      <td>-0.190733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14615</th>\n",
       "      <td>86</td>\n",
       "      <td>47</td>\n",
       "      <td>448</td>\n",
       "      <td>16</td>\n",
       "      <td>41</td>\n",
       "      <td>266</td>\n",
       "      <td>-0.340769</td>\n",
       "      <td>-0.058809</td>\n",
       "      <td>-1.138488</td>\n",
       "      <td>-0.384569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.681083</td>\n",
       "      <td>0.017155</td>\n",
       "      <td>0.198969</td>\n",
       "      <td>0.235866</td>\n",
       "      <td>-0.116819</td>\n",
       "      <td>-0.105489</td>\n",
       "      <td>-0.097344</td>\n",
       "      <td>-0.315073</td>\n",
       "      <td>0.851146</td>\n",
       "      <td>-0.265891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14616</th>\n",
       "      <td>260</td>\n",
       "      <td>139</td>\n",
       "      <td>1363</td>\n",
       "      <td>35</td>\n",
       "      <td>126</td>\n",
       "      <td>845</td>\n",
       "      <td>-0.298359</td>\n",
       "      <td>-0.085764</td>\n",
       "      <td>-1.240260</td>\n",
       "      <td>-0.396023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803427</td>\n",
       "      <td>0.030206</td>\n",
       "      <td>0.115804</td>\n",
       "      <td>0.236635</td>\n",
       "      <td>-0.126242</td>\n",
       "      <td>-0.068015</td>\n",
       "      <td>-0.212386</td>\n",
       "      <td>-0.343189</td>\n",
       "      <td>0.971015</td>\n",
       "      <td>-0.242007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14617 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_count  num_stop_words  num_chars  num_punctuation_chars  \\\n",
       "0              20               4        111                      9   \n",
       "1               6               0         29                      6   \n",
       "2              28               5        155                      5   \n",
       "3              11               2         49                      6   \n",
       "4              13               6         61                      2   \n",
       "...           ...             ...        ...                    ...   \n",
       "14612         131              59        745                     24   \n",
       "14613         121              62        670                     18   \n",
       "14614         141              72        770                      9   \n",
       "14615          86              47        448                     16   \n",
       "14616         260             139       1363                     35   \n",
       "\n",
       "       word_count_after_preprocessing  num_chars_after_preprocessing  \\\n",
       "0                                  16                             82   \n",
       "1                                   6                             23   \n",
       "2                                  20                            110   \n",
       "3                                   9                             35   \n",
       "4                                   8                             40   \n",
       "...                               ...                            ...   \n",
       "14612                              74                            517   \n",
       "14613                              63                            430   \n",
       "14614                              69                            490   \n",
       "14615                              41                            266   \n",
       "14616                             126                            845   \n",
       "\n",
       "       vector_dim_1  vector_dim_2  vector_dim_3  vector_dim_4  ...  \\\n",
       "0         -0.445367     -0.101509     -1.050856     -0.263047  ...   \n",
       "1         -0.336330     -0.172123     -0.879673     -0.000336  ...   \n",
       "2         -0.234010     -0.005789     -0.754281      0.017621  ...   \n",
       "3         -0.360958     -0.032422     -1.517050     -0.111853  ...   \n",
       "4         -0.108012     -0.021497     -1.344048     -0.605199  ...   \n",
       "...             ...           ...           ...           ...  ...   \n",
       "14612     -0.251930     -0.063167     -1.194805     -0.387192  ...   \n",
       "14613     -0.188276     -0.060201     -1.234956     -0.394533  ...   \n",
       "14614     -0.237662     -0.076631     -1.247667     -0.374296  ...   \n",
       "14615     -0.340769     -0.058809     -1.138488     -0.384569  ...   \n",
       "14616     -0.298359     -0.085764     -1.240260     -0.396023  ...   \n",
       "\n",
       "       vector_dim_91  vector_dim_92  vector_dim_93  vector_dim_94  \\\n",
       "0           0.363760       0.423471      -0.183125       0.123134   \n",
       "1           0.178105       0.469185      -0.159354       0.181287   \n",
       "2           0.179313       0.245597      -0.074740       0.326302   \n",
       "3           0.446681       0.821034      -0.187948       0.338291   \n",
       "4           0.748294      -0.040061      -0.212751       0.148267   \n",
       "...              ...            ...            ...            ...   \n",
       "14612       0.651287      -0.020689       0.026659       0.290871   \n",
       "14613       0.824768      -0.053670       0.166092       0.286987   \n",
       "14614       0.801583      -0.036325       0.300470       0.293482   \n",
       "14615       0.681083       0.017155       0.198969       0.235866   \n",
       "14616       0.803427       0.030206       0.115804       0.236635   \n",
       "\n",
       "       vector_dim_95  vector_dim_96  vector_dim_97  vector_dim_98  \\\n",
       "0           0.421487      -0.085384       0.096360      -0.311313   \n",
       "1           0.370253      -0.151212      -0.029242       0.017299   \n",
       "2           0.246354       0.009656      -0.045136      -0.300550   \n",
       "3           0.571384      -0.449642      -0.185700       0.029383   \n",
       "4          -0.139923      -0.006358       0.034125      -0.220026   \n",
       "...              ...            ...            ...            ...   \n",
       "14612       0.011417       0.002676      -0.094306      -0.413245   \n",
       "14613      -0.115114      -0.072424      -0.317539      -0.333784   \n",
       "14614      -0.156067      -0.117382      -0.343503      -0.324489   \n",
       "14615      -0.116819      -0.105489      -0.097344      -0.315073   \n",
       "14616      -0.126242      -0.068015      -0.212386      -0.343189   \n",
       "\n",
       "       vector_dim_99  vector_dim_100  \n",
       "0           0.590059       -0.068110  \n",
       "1           0.309134       -0.183176  \n",
       "2           0.424266       -0.487235  \n",
       "3           0.535742       -0.069742  \n",
       "4           1.054043       -0.075081  \n",
       "...              ...             ...  \n",
       "14612       0.968019       -0.213552  \n",
       "14613       0.966461       -0.284511  \n",
       "14614       0.929325       -0.190733  \n",
       "14615       0.851146       -0.265891  \n",
       "14616       0.971015       -0.242007  \n",
       "\n",
       "[14617 rows x 106 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=final_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        1\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "14612    0\n",
       "14613    0\n",
       "14614    0\n",
       "14615    0\n",
       "14616    0\n",
       "Name: label, Length: 14617, dtype: int32"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=500)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=500)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "LR = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Train the model\n",
    "LR.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the scaled test set\n",
    "y_pred = LR.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[2148  107]\n",
      " [ 206  463]]\n"
     ]
    }
   ],
   "source": [
    "# Print confusion matrix and classification report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93      2255\n",
      "           1       0.81      0.69      0.75       669\n",
      "\n",
      "    accuracy                           0.89      2924\n",
      "   macro avg       0.86      0.82      0.84      2924\n",
      "weighted avg       0.89      0.89      0.89      2924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6236547913787331"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_proba = LR.predict_proba(X_test)\n",
    "roc_auc_score(y_test, y_pred_proba[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3962653992622275"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.238593\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  label   No. Observations:                11693\n",
      "Model:                          Logit   Df Residuals:                    11586\n",
      "Method:                           MLE   Df Model:                          106\n",
      "Date:                Wed, 30 Oct 2024   Pseudo R-squ.:                  0.5550\n",
      "Time:                        13:19:51   Log-Likelihood:                -2789.9\n",
      "converged:                       True   LL-Null:                       -6270.0\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==================================================================================================\n",
      "                                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "const                             -0.6502      0.165     -3.940      0.000      -0.974      -0.327\n",
      "word_count                         0.0007      0.002      0.287      0.774      -0.004       0.005\n",
      "num_stop_words                     0.0002      0.002      0.082      0.934      -0.004       0.005\n",
      "num_chars                         -0.0010      0.001     -0.934      0.350      -0.003       0.001\n",
      "num_punctuation_chars              0.0017      0.002      1.030      0.303      -0.002       0.005\n",
      "word_count_after_preprocessing     0.0022      0.002      0.920      0.358      -0.002       0.007\n",
      "num_chars_after_preprocessing      0.0009      0.001      0.896      0.370      -0.001       0.003\n",
      "vector_dim_1                      -4.1892      4.259     -0.984      0.325     -12.537       4.159\n",
      "vector_dim_2                      -3.7161      4.960     -0.749      0.454     -13.438       6.005\n",
      "vector_dim_3                       4.5247      4.479      1.010      0.312      -4.254      13.303\n",
      "vector_dim_4                      14.7807      4.003      3.692      0.000       6.934      22.627\n",
      "vector_dim_5                       1.5056      5.103      0.295      0.768      -8.495      11.507\n",
      "vector_dim_6                     -15.4669      4.800     -3.223      0.001     -24.874      -6.060\n",
      "vector_dim_7                       7.9902      4.630      1.726      0.084      -1.084      17.064\n",
      "vector_dim_8                       2.2658      4.979      0.455      0.649      -7.493      12.025\n",
      "vector_dim_9                      -4.5495      4.991     -0.912      0.362     -14.331       5.232\n",
      "vector_dim_10                      4.3537      4.254      1.023      0.306      -3.984      12.691\n",
      "vector_dim_11                     -7.3647      4.755     -1.549      0.121     -16.684       1.955\n",
      "vector_dim_12                     24.6360      4.251      5.795      0.000      16.304      32.968\n",
      "vector_dim_13                    -15.2096      4.583     -3.319      0.001     -24.192      -6.227\n",
      "vector_dim_14                     21.1678      4.642      4.560      0.000      12.070      30.265\n",
      "vector_dim_15                     16.1554      4.648      3.476      0.001       7.046      25.265\n",
      "vector_dim_16                     12.5991      6.004      2.098      0.036       0.831      24.367\n",
      "vector_dim_17                     -2.4735      5.243     -0.472      0.637     -12.750       7.803\n",
      "vector_dim_18                     17.0869      3.761      4.543      0.000       9.715      24.459\n",
      "vector_dim_19                     29.0199      5.204      5.576      0.000      18.820      39.220\n",
      "vector_dim_20                     24.8038      4.511      5.498      0.000      15.962      33.646\n",
      "vector_dim_21                      8.6433      3.697      2.338      0.019       1.398      15.888\n",
      "vector_dim_22                    -10.1081      4.464     -2.264      0.024     -18.857      -1.359\n",
      "vector_dim_23                    -16.4283      4.238     -3.876      0.000     -24.735      -8.122\n",
      "vector_dim_24                    -10.5233      4.378     -2.404      0.016     -19.105      -1.942\n",
      "vector_dim_25                    -10.1287      4.372     -2.317      0.021     -18.698      -1.559\n",
      "vector_dim_26                    -13.2824      5.084     -2.613      0.009     -23.247      -3.318\n",
      "vector_dim_27                     17.5302      4.716      3.717      0.000       8.287      26.773\n",
      "vector_dim_28                    -27.6126      4.476     -6.168      0.000     -36.386     -18.839\n",
      "vector_dim_29                     19.4981      4.689      4.158      0.000      10.307      28.689\n",
      "vector_dim_30                     14.0770      5.044      2.791      0.005       4.190      23.964\n",
      "vector_dim_31                    -13.2518      4.165     -3.181      0.001     -21.416      -5.088\n",
      "vector_dim_32                     28.0502      5.575      5.031      0.000      17.123      38.978\n",
      "vector_dim_33                     -7.2818      4.847     -1.502      0.133     -16.782       2.218\n",
      "vector_dim_34                    -12.9687      4.801     -2.701      0.007     -22.378      -3.559\n",
      "vector_dim_35                    -19.8606      5.141     -3.863      0.000     -29.937      -9.785\n",
      "vector_dim_36                    -21.5360      4.591     -4.691      0.000     -30.535     -12.537\n",
      "vector_dim_37                    -11.0306      4.556     -2.421      0.015     -19.960      -2.101\n",
      "vector_dim_38                     16.7751      4.300      3.901      0.000       8.347      25.203\n",
      "vector_dim_39                     -8.7991      4.638     -1.897      0.058     -17.888       0.290\n",
      "vector_dim_40                    -27.7441      4.064     -6.826      0.000     -35.710     -19.778\n",
      "vector_dim_41                      0.1734      4.563      0.038      0.970      -8.770       9.117\n",
      "vector_dim_42                      7.8039      5.181      1.506      0.132      -2.351      17.958\n",
      "vector_dim_43                     -5.3905      4.686     -1.150      0.250     -14.575       3.794\n",
      "vector_dim_44                     -6.8362      4.602     -1.485      0.137     -15.856       2.184\n",
      "vector_dim_45                     23.9127      6.265      3.817      0.000      11.633      36.192\n",
      "vector_dim_46                    -13.8513      4.962     -2.791      0.005     -23.577      -4.125\n",
      "vector_dim_47                     17.2819      4.202      4.112      0.000       9.046      25.518\n",
      "vector_dim_48                      3.2801      5.245      0.625      0.532      -7.000      13.561\n",
      "vector_dim_49                     -0.9109      5.141     -0.177      0.859     -10.987       9.165\n",
      "vector_dim_50                     30.6787      4.798      6.394      0.000      21.275      40.082\n",
      "vector_dim_51                      7.6663      4.370      1.754      0.079      -0.900      16.232\n",
      "vector_dim_52                     10.5439      4.629      2.278      0.023       1.471      19.617\n",
      "vector_dim_53                     18.9504      4.657      4.069      0.000       9.823      28.078\n",
      "vector_dim_54                     -9.7836      5.674     -1.724      0.085     -20.904       1.337\n",
      "vector_dim_55                     -6.8903      4.354     -1.583      0.114     -15.424       1.643\n",
      "vector_dim_56                      3.1827      4.299      0.740      0.459      -5.242      11.608\n",
      "vector_dim_57                     33.0055      4.043      8.163      0.000      25.081      40.930\n",
      "vector_dim_58                    -23.5693      5.238     -4.499      0.000     -33.836     -13.303\n",
      "vector_dim_59                      2.1777      4.239      0.514      0.607      -6.130      10.486\n",
      "vector_dim_60                     18.4912      4.017      4.603      0.000      10.618      26.365\n",
      "vector_dim_61                    -12.4672      5.323     -2.342      0.019     -22.900      -2.035\n",
      "vector_dim_62                     -2.2959      5.368     -0.428      0.669     -12.817       8.225\n",
      "vector_dim_63                      5.3822      4.283      1.257      0.209      -3.013      13.778\n",
      "vector_dim_64                      3.2309      4.623      0.699      0.485      -5.831      12.292\n",
      "vector_dim_65                     -1.4010      3.926     -0.357      0.721      -9.096       6.294\n",
      "vector_dim_66                     22.1226      4.698      4.709      0.000      12.915      31.330\n",
      "vector_dim_67                     14.1031      3.960      3.561      0.000       6.342      21.864\n",
      "vector_dim_68                    -12.3624      4.607     -2.683      0.007     -21.393      -3.332\n",
      "vector_dim_69                     -1.0846      4.875     -0.222      0.824     -10.640       8.471\n",
      "vector_dim_70                     -8.9528      4.718     -1.897      0.058     -18.201       0.295\n",
      "vector_dim_71                      0.5004      4.862      0.103      0.918      -9.029      10.030\n",
      "vector_dim_72                     26.2757      4.510      5.826      0.000      17.437      35.115\n",
      "vector_dim_73                    -27.3711      5.246     -5.217      0.000     -37.653     -17.089\n",
      "vector_dim_74                    -13.4656      4.886     -2.756      0.006     -23.043      -3.888\n",
      "vector_dim_75                    -15.7117      4.277     -3.673      0.000     -24.095      -7.328\n",
      "vector_dim_76                     22.5173      4.069      5.534      0.000      14.542      30.493\n",
      "vector_dim_77                    -13.9557      4.904     -2.846      0.004     -23.567      -4.345\n",
      "vector_dim_78                    -32.9670      5.380     -6.128      0.000     -43.512     -22.422\n",
      "vector_dim_79                     17.6287      4.810      3.665      0.000       8.201      27.056\n",
      "vector_dim_80                     18.8360      4.269      4.412      0.000      10.468      27.204\n",
      "vector_dim_81                    -10.7703      5.190     -2.075      0.038     -20.942      -0.599\n",
      "vector_dim_82                     -1.4120      4.791     -0.295      0.768     -10.803       7.979\n",
      "vector_dim_83                     11.5499      4.024      2.870      0.004       3.663      19.437\n",
      "vector_dim_84                     17.2302      4.631      3.720      0.000       8.153      26.307\n",
      "vector_dim_85                     -2.2478      4.892     -0.460      0.646     -11.836       7.340\n",
      "vector_dim_86                      0.1146      4.925      0.023      0.981      -9.537       9.766\n",
      "vector_dim_87                     -6.4048      4.997     -1.282      0.200     -16.200       3.390\n",
      "vector_dim_88                      3.8217      4.348      0.879      0.379      -4.700      12.343\n",
      "vector_dim_89                     -6.0456      4.163     -1.452      0.146     -14.205       2.114\n",
      "vector_dim_90                     -5.7620      4.456     -1.293      0.196     -14.496       2.972\n",
      "vector_dim_91                     12.7351      3.891      3.273      0.001       5.109      20.361\n",
      "vector_dim_92                     -3.5861      4.685     -0.765      0.444     -12.769       5.597\n",
      "vector_dim_93                     -7.7799      4.836     -1.609      0.108     -17.257       1.697\n",
      "vector_dim_94                      9.1723      4.501      2.038      0.042       0.350      17.994\n",
      "vector_dim_95                      8.2663      4.928      1.677      0.093      -1.393      17.926\n",
      "vector_dim_96                     15.6339      4.570      3.421      0.001       6.677      24.590\n",
      "vector_dim_97                    -16.6194      4.553     -3.651      0.000     -25.542      -7.696\n",
      "vector_dim_98                     -1.7431      5.261     -0.331      0.740     -12.055       8.569\n",
      "vector_dim_99                    -17.5484      3.712     -4.728      0.000     -24.824     -10.273\n",
      "vector_dim_100                     1.1710      4.723      0.248      0.804      -8.086      10.428\n",
      "==================================================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.14 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = sm.add_constant(X_train)  # Add a constant term (intercept)\n",
    "model = sm.Logit(y_train, X).fit()  \n",
    "print(model.summary())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. **Model Information:**\n",
    "   - **Dependent Variable (`Dep. Variable`)**: `label`, which refers to the target or outcome variable you're predicting.\n",
    "   - **Number of Observations (`No. Observations`)**: 11,693, indicating the number of samples used to train the model.\n",
    "   - **Model (`Model`)**: The logistic regression model, which uses Maximum Likelihood Estimation (`MLE`).\n",
    "   - **Residual Degrees of Freedom (`Df Residuals`)**: 11,586, which is the difference between the number of observations and the number of model parameters (106).\n",
    "   - **Pseudo R-squared (`Pseudo R-squ.`)**: 0.5550, a goodness-of-fit measure. This suggests that around **55.50%** of the variance in the target variable is explained by the model.\n",
    "\n",
    "### 2. **Log-Likelihood Values:**\n",
    "   - **Current function value (Log-Likelihood)**: -2,789.9, which is the final value of the log-likelihood after optimization. Lower values indicate a better fit of the model.\n",
    "   - **LL-Null**: -6,270.0, the log-likelihood for a model with no predictors (only the intercept).\n",
    "   - **LLR p-value**: 0.000, which indicates that the model as a whole is statistically significant compared to a null model.\n",
    "\n",
    "### 3. **Coefficients and Interpretation:**\n",
    "   For each feature, the model provides:\n",
    "   - **Coefficient (`coef`)**: The impact of that feature on the log-odds of the outcome. A positive coefficient increases the odds of the event happening, while a negative coefficient decreases it.\n",
    "   - **Standard Error (`std err`)**: The variability of the coefficient estimate.\n",
    "   - **Z-value (`z`)**: The test statistic for testing whether the coefficient is different from 0.\n",
    "   - **P>|z|**: The p-value, showing the statistical significance of the coefficient. Typically, a p-value below 0.05 indicates that the feature is statistically significant.\n",
    "   - **Confidence Interval**: The 95% confidence interval for the coefficient, which gives a range within which the true value of the coefficient likely lies.\n",
    "\n",
    "   Let's interpret some specific features:\n",
    "   - **`const`**: The intercept has a significant negative value (-0.6502) with a p-value of 0.000, meaning it’s statistically significant.\n",
    "   - **`word_count`**: The coefficient (0.0007) is positive, but the p-value (0.774) is large, indicating that this feature is not statistically significant.\n",
    "   - **`num_chars`**: The coefficient (-0.0010) suggests a negative impact, but with a p-value of 0.350, it's not statistically significant.\n",
    "   - **`vector_dim_1`**: The coefficient (-4.1892) is large in magnitude but has a high p-value (0.325), meaning it’s also not statistically significant.\n",
    "\n",
    "   Most of the features shown have **high p-values** (above 0.05), indicating they are **not statistically significant** predictors in this model.\n",
    "\n",
    "### 4. **Quasi-Separation Warning:**\n",
    "   - The message **\"Possibly complete quasi-separation\"** suggests that a fraction (14%) of observations are perfectly predicted by the model. This can lead to **complete quasi-separation**, meaning that some features can perfectly predict the outcome for certain samples.\n",
    "   - **Implications**: This might cause instability in the estimated coefficients, especially with large positive or negative values. It also suggests that some parameters may not be identified, meaning their effects can’t be accurately estimated.\n",
    "\n",
    "### 5. **Overall Model Fit and Significance:**\n",
    "   - The model as a whole is significant, as indicated by the LLR p-value of 0.000.\n",
    "   - However, most individual features do not appear to be significant, suggesting that they may not contribute much predictive power individually.\n",
    "   - The Pseudo R-squared of 0.555 indicates a decent fit, but there's room for improvement in explaining the variability in the outcome.\n",
    "\n",
    "### Next Steps:\n",
    "   - You may want to assess whether some features are collinear or unnecessary and try simplifying the model by removing insignificant predictors.\n",
    "   - Address the **quasi-separation issue** by possibly removing or regularizing the features causing this, or using penalized logistic regression (e.g., Lasso or Ridge).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>num_stop_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_punctuation_chars</th>\n",
       "      <th>word_count_after_preprocessing</th>\n",
       "      <th>num_chars_after_preprocessing</th>\n",
       "      <th>vector_dim_1</th>\n",
       "      <th>vector_dim_2</th>\n",
       "      <th>vector_dim_3</th>\n",
       "      <th>vector_dim_4</th>\n",
       "      <th>...</th>\n",
       "      <th>vector_dim_91</th>\n",
       "      <th>vector_dim_92</th>\n",
       "      <th>vector_dim_93</th>\n",
       "      <th>vector_dim_94</th>\n",
       "      <th>vector_dim_95</th>\n",
       "      <th>vector_dim_96</th>\n",
       "      <th>vector_dim_97</th>\n",
       "      <th>vector_dim_98</th>\n",
       "      <th>vector_dim_99</th>\n",
       "      <th>vector_dim_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>111</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>82</td>\n",
       "      <td>-0.445367</td>\n",
       "      <td>-0.101509</td>\n",
       "      <td>-1.050856</td>\n",
       "      <td>-0.263047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363760</td>\n",
       "      <td>0.423471</td>\n",
       "      <td>-0.183125</td>\n",
       "      <td>0.123134</td>\n",
       "      <td>0.421487</td>\n",
       "      <td>-0.085384</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>-0.311313</td>\n",
       "      <td>0.590059</td>\n",
       "      <td>-0.068110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.336330</td>\n",
       "      <td>-0.172123</td>\n",
       "      <td>-0.879673</td>\n",
       "      <td>-0.000336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178105</td>\n",
       "      <td>0.469185</td>\n",
       "      <td>-0.159354</td>\n",
       "      <td>0.181287</td>\n",
       "      <td>0.370253</td>\n",
       "      <td>-0.151212</td>\n",
       "      <td>-0.029242</td>\n",
       "      <td>0.017299</td>\n",
       "      <td>0.309134</td>\n",
       "      <td>-0.183176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>110</td>\n",
       "      <td>-0.234010</td>\n",
       "      <td>-0.005789</td>\n",
       "      <td>-0.754281</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179313</td>\n",
       "      <td>0.245597</td>\n",
       "      <td>-0.074740</td>\n",
       "      <td>0.326302</td>\n",
       "      <td>0.246354</td>\n",
       "      <td>0.009656</td>\n",
       "      <td>-0.045136</td>\n",
       "      <td>-0.300550</td>\n",
       "      <td>0.424266</td>\n",
       "      <td>-0.487235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.360958</td>\n",
       "      <td>-0.032422</td>\n",
       "      <td>-1.517050</td>\n",
       "      <td>-0.111853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446681</td>\n",
       "      <td>0.821034</td>\n",
       "      <td>-0.187948</td>\n",
       "      <td>0.338291</td>\n",
       "      <td>0.571384</td>\n",
       "      <td>-0.449642</td>\n",
       "      <td>-0.185700</td>\n",
       "      <td>0.029383</td>\n",
       "      <td>0.535742</td>\n",
       "      <td>-0.069742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>-0.108012</td>\n",
       "      <td>-0.021497</td>\n",
       "      <td>-1.344048</td>\n",
       "      <td>-0.605199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748294</td>\n",
       "      <td>-0.040061</td>\n",
       "      <td>-0.212751</td>\n",
       "      <td>0.148267</td>\n",
       "      <td>-0.139923</td>\n",
       "      <td>-0.006358</td>\n",
       "      <td>0.034125</td>\n",
       "      <td>-0.220026</td>\n",
       "      <td>1.054043</td>\n",
       "      <td>-0.075081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14612</th>\n",
       "      <td>131</td>\n",
       "      <td>59</td>\n",
       "      <td>745</td>\n",
       "      <td>24</td>\n",
       "      <td>74</td>\n",
       "      <td>517</td>\n",
       "      <td>-0.251930</td>\n",
       "      <td>-0.063167</td>\n",
       "      <td>-1.194805</td>\n",
       "      <td>-0.387192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651287</td>\n",
       "      <td>-0.020689</td>\n",
       "      <td>0.026659</td>\n",
       "      <td>0.290871</td>\n",
       "      <td>0.011417</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>-0.094306</td>\n",
       "      <td>-0.413245</td>\n",
       "      <td>0.968019</td>\n",
       "      <td>-0.213552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14613</th>\n",
       "      <td>121</td>\n",
       "      <td>62</td>\n",
       "      <td>670</td>\n",
       "      <td>18</td>\n",
       "      <td>63</td>\n",
       "      <td>430</td>\n",
       "      <td>-0.188276</td>\n",
       "      <td>-0.060201</td>\n",
       "      <td>-1.234956</td>\n",
       "      <td>-0.394533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.824768</td>\n",
       "      <td>-0.053670</td>\n",
       "      <td>0.166092</td>\n",
       "      <td>0.286987</td>\n",
       "      <td>-0.115114</td>\n",
       "      <td>-0.072424</td>\n",
       "      <td>-0.317539</td>\n",
       "      <td>-0.333784</td>\n",
       "      <td>0.966461</td>\n",
       "      <td>-0.284511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14614</th>\n",
       "      <td>141</td>\n",
       "      <td>72</td>\n",
       "      <td>770</td>\n",
       "      <td>9</td>\n",
       "      <td>69</td>\n",
       "      <td>490</td>\n",
       "      <td>-0.237662</td>\n",
       "      <td>-0.076631</td>\n",
       "      <td>-1.247667</td>\n",
       "      <td>-0.374296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.801583</td>\n",
       "      <td>-0.036325</td>\n",
       "      <td>0.300470</td>\n",
       "      <td>0.293482</td>\n",
       "      <td>-0.156067</td>\n",
       "      <td>-0.117382</td>\n",
       "      <td>-0.343503</td>\n",
       "      <td>-0.324489</td>\n",
       "      <td>0.929325</td>\n",
       "      <td>-0.190733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14615</th>\n",
       "      <td>86</td>\n",
       "      <td>47</td>\n",
       "      <td>448</td>\n",
       "      <td>16</td>\n",
       "      <td>41</td>\n",
       "      <td>266</td>\n",
       "      <td>-0.340769</td>\n",
       "      <td>-0.058809</td>\n",
       "      <td>-1.138488</td>\n",
       "      <td>-0.384569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.681083</td>\n",
       "      <td>0.017155</td>\n",
       "      <td>0.198969</td>\n",
       "      <td>0.235866</td>\n",
       "      <td>-0.116819</td>\n",
       "      <td>-0.105489</td>\n",
       "      <td>-0.097344</td>\n",
       "      <td>-0.315073</td>\n",
       "      <td>0.851146</td>\n",
       "      <td>-0.265891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14616</th>\n",
       "      <td>260</td>\n",
       "      <td>139</td>\n",
       "      <td>1363</td>\n",
       "      <td>35</td>\n",
       "      <td>126</td>\n",
       "      <td>845</td>\n",
       "      <td>-0.298359</td>\n",
       "      <td>-0.085764</td>\n",
       "      <td>-1.240260</td>\n",
       "      <td>-0.396023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803427</td>\n",
       "      <td>0.030206</td>\n",
       "      <td>0.115804</td>\n",
       "      <td>0.236635</td>\n",
       "      <td>-0.126242</td>\n",
       "      <td>-0.068015</td>\n",
       "      <td>-0.212386</td>\n",
       "      <td>-0.343189</td>\n",
       "      <td>0.971015</td>\n",
       "      <td>-0.242007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14617 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_count  num_stop_words  num_chars  num_punctuation_chars  \\\n",
       "0              20               4        111                      9   \n",
       "1               6               0         29                      6   \n",
       "2              28               5        155                      5   \n",
       "3              11               2         49                      6   \n",
       "4              13               6         61                      2   \n",
       "...           ...             ...        ...                    ...   \n",
       "14612         131              59        745                     24   \n",
       "14613         121              62        670                     18   \n",
       "14614         141              72        770                      9   \n",
       "14615          86              47        448                     16   \n",
       "14616         260             139       1363                     35   \n",
       "\n",
       "       word_count_after_preprocessing  num_chars_after_preprocessing  \\\n",
       "0                                  16                             82   \n",
       "1                                   6                             23   \n",
       "2                                  20                            110   \n",
       "3                                   9                             35   \n",
       "4                                   8                             40   \n",
       "...                               ...                            ...   \n",
       "14612                              74                            517   \n",
       "14613                              63                            430   \n",
       "14614                              69                            490   \n",
       "14615                              41                            266   \n",
       "14616                             126                            845   \n",
       "\n",
       "       vector_dim_1  vector_dim_2  vector_dim_3  vector_dim_4  ...  \\\n",
       "0         -0.445367     -0.101509     -1.050856     -0.263047  ...   \n",
       "1         -0.336330     -0.172123     -0.879673     -0.000336  ...   \n",
       "2         -0.234010     -0.005789     -0.754281      0.017621  ...   \n",
       "3         -0.360958     -0.032422     -1.517050     -0.111853  ...   \n",
       "4         -0.108012     -0.021497     -1.344048     -0.605199  ...   \n",
       "...             ...           ...           ...           ...  ...   \n",
       "14612     -0.251930     -0.063167     -1.194805     -0.387192  ...   \n",
       "14613     -0.188276     -0.060201     -1.234956     -0.394533  ...   \n",
       "14614     -0.237662     -0.076631     -1.247667     -0.374296  ...   \n",
       "14615     -0.340769     -0.058809     -1.138488     -0.384569  ...   \n",
       "14616     -0.298359     -0.085764     -1.240260     -0.396023  ...   \n",
       "\n",
       "       vector_dim_91  vector_dim_92  vector_dim_93  vector_dim_94  \\\n",
       "0           0.363760       0.423471      -0.183125       0.123134   \n",
       "1           0.178105       0.469185      -0.159354       0.181287   \n",
       "2           0.179313       0.245597      -0.074740       0.326302   \n",
       "3           0.446681       0.821034      -0.187948       0.338291   \n",
       "4           0.748294      -0.040061      -0.212751       0.148267   \n",
       "...              ...            ...            ...            ...   \n",
       "14612       0.651287      -0.020689       0.026659       0.290871   \n",
       "14613       0.824768      -0.053670       0.166092       0.286987   \n",
       "14614       0.801583      -0.036325       0.300470       0.293482   \n",
       "14615       0.681083       0.017155       0.198969       0.235866   \n",
       "14616       0.803427       0.030206       0.115804       0.236635   \n",
       "\n",
       "       vector_dim_95  vector_dim_96  vector_dim_97  vector_dim_98  \\\n",
       "0           0.421487      -0.085384       0.096360      -0.311313   \n",
       "1           0.370253      -0.151212      -0.029242       0.017299   \n",
       "2           0.246354       0.009656      -0.045136      -0.300550   \n",
       "3           0.571384      -0.449642      -0.185700       0.029383   \n",
       "4          -0.139923      -0.006358       0.034125      -0.220026   \n",
       "...              ...            ...            ...            ...   \n",
       "14612       0.011417       0.002676      -0.094306      -0.413245   \n",
       "14613      -0.115114      -0.072424      -0.317539      -0.333784   \n",
       "14614      -0.156067      -0.117382      -0.343503      -0.324489   \n",
       "14615      -0.116819      -0.105489      -0.097344      -0.315073   \n",
       "14616      -0.126242      -0.068015      -0.212386      -0.343189   \n",
       "\n",
       "       vector_dim_99  vector_dim_100  \n",
       "0           0.590059       -0.068110  \n",
       "1           0.309134       -0.183176  \n",
       "2           0.424266       -0.487235  \n",
       "3           0.535742       -0.069742  \n",
       "4           1.054043       -0.075081  \n",
       "...              ...             ...  \n",
       "14612       0.968019       -0.213552  \n",
       "14613       0.966461       -0.284511  \n",
       "14614       0.929325       -0.190733  \n",
       "14615       0.851146       -0.265891  \n",
       "14616       0.971015       -0.242007  \n",
       "\n",
       "[14617 rows x 106 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        1\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "14612    0\n",
       "14613    0\n",
       "14614    0\n",
       "14615    0\n",
       "14616    0\n",
       "Name: label, Length: 14617, dtype: int32"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97      2255\n",
      "           1       0.94      0.84      0.89       669\n",
      "\n",
      "    accuracy                           0.95      2924\n",
      "   macro avg       0.95      0.91      0.93      2924\n",
      "weighted avg       0.95      0.95      0.95      2924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initializing the Random Forest model with class_weight='balanced'\n",
    "rf_model = RandomForestClassifier(n_estimators=100,class_weight='balanced', random_state=42)\n",
    "\n",
    "# Fitting the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[2219   36]\n",
      " [ 105  564]]\n"
     ]
    }
   ],
   "source": [
    "# Print confusion matrix and classification report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014995741070333664"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba = rf_model.predict_proba(X_test)\n",
    "roc_auc_score(y_test, y_pred_proba[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9850042589296664"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred_proba[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(random_state=42)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "# Train the model\n",
    "dt_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = dt_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2122  133]\n",
      " [ 138  531]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94      2255\n",
      "           1       0.80      0.79      0.80       669\n",
      "\n",
      "    accuracy                           0.91      2924\n",
      "   macro avg       0.87      0.87      0.87      2924\n",
      "weighted avg       0.91      0.91      0.91      2924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1326290356258638"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba = dt_classifier.predict_proba(X_test)\n",
    "roc_auc_score(y_test, y_pred_proba[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8673709643741361"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred_proba[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example: X is your features and y is your target\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Calculate scale_pos_weight (ratio of majority class to minority class)\n",
    "ratio = sum(y_train == 0) / sum(y_train == 1)\n",
    "\n",
    "# Create the XGBoost model\n",
    "XG_Boost_model = XGBClassifier(\n",
    "    scale_pos_weight=ratio,  # Handle class imbalance\n",
    "    n_estimators=100,        # Number of trees\n",
    "    max_depth=4,             # Maximum depth of the trees\n",
    "    learning_rate=0.1,       # Step size shrinkage\n",
    "    objective='binary:logistic',  # Binary classification\n",
    "    random_state=42,         # Random state for reproducibility\n",
    "    eval_metric='logloss'    # Evaluation metric for binary classification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3958646616541355"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=4,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=100,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=4,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=100,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='logloss',\n",
       "              feature_types=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=4,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=100,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=42, ...)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "XG_Boost_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the test set\n",
    "y_pred = XG_Boost_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      2255\n",
      "           1       0.83      0.93      0.88       669\n",
      "\n",
      "    accuracy                           0.94      2924\n",
      "   macro avg       0.91      0.94      0.92      2924\n",
      "weighted avg       0.95      0.94      0.94      2924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2130  125]\n",
      " [  44  625]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Positives (TP) = 625:\n",
    "These are cases where the model correctly predicted email as spam (positive class).\n",
    "\n",
    "True Negatives (TN) = 2130:\n",
    "These are cases where the model correctly predicted email as ham (negative class).\n",
    "\n",
    "False Positives (FP) = 125:\n",
    "These are cases where the model incorrectly predicted email as spam (positive class) when the actual email was ham.\n",
    "\n",
    "False Negatives (FN) = 44:\n",
    "These are cases where the model incorrectly predicted email as ham (negative class) when the actual email was spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.01507627958464664\n"
     ]
    }
   ],
   "source": [
    "# AUC-ROC score\n",
    "y_pred_proba = XG_Boost_model.predict_proba(X_test)[:, 0]\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test, y_pred_proba)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.9849237204153534\n"
     ]
    }
   ],
   "source": [
    "# AUC-ROC score\n",
    "y_pred_proba = XG_Boost_model.predict_proba(X_test)[:, 1]\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test, y_pred_proba)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-learner\n",
    "knn_meta_learner = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97      2255\n",
      "           1       0.91      0.86      0.89       669\n",
      "\n",
      "    accuracy                           0.95      2924\n",
      "   macro avg       0.94      0.92      0.93      2924\n",
      "weighted avg       0.95      0.95      0.95      2924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the StackingClassifier with RandomForestClassifier and XGBoostClassifier as base models, and KNN as meta-learner\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('RandomForestClassifier', rf_model),\n",
    "        ('XGBoostClassifier', XG_Boost_model)\n",
    "    ],\n",
    "    final_estimator=knn_meta_learner,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Train the stacking model\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2200,   55],\n",
       "       [  91,  578]], dtype=int64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.0331994339103603\n"
     ]
    }
   ],
   "source": [
    "# AUC-ROC score\n",
    "y_pred_proba = stacking_model.predict_proba(X_test)[:, 0]\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test, y_pred_proba)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.9668005660896398\n"
     ]
    }
   ],
   "source": [
    "# AUC-ROC score\n",
    "y_pred_proba = stacking_model.predict_proba(X_test)[:, 1]\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test, y_pred_proba)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Assume you have your trained models in a dictionary\n",
    "models = {\n",
    "    'Logistic Regression': LR,\n",
    "    'Random Forest': rf_model,\n",
    "    'XGBClassifier': XG_Boost_model,\n",
    "    'Decision Tree Classifier': dt_classifier,\n",
    "    'Stacking Model': stacking_model\n",
    "}\n",
    "\n",
    "# Initialize lists to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through each model to evaluate\n",
    "for model_name, model in models.items():\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)  # Ensure X_test is defined\n",
    "    y_true = y_test  # Ensure y_test is defined\n",
    "\n",
    "    # Get classification report\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "    # Get confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Extract relevant metrics safely\n",
    "    accuracy = report['accuracy']\n",
    "    precision = report['1']['precision'] if '1' in report else 0.0  # Handle missing class\n",
    "    recall = report['1']['recall'] if '1' in report else 0.0  # Handle missing class\n",
    "    f1_score = report['1']['f1-score'] if '1' in report else 0.0  # Handle missing class\n",
    "    type_1_error = cm[0][1]  # False Positive\n",
    "    type_2_error = cm[1][0]  # False Negative\n",
    "\n",
    "    # Append the results\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1_score,\n",
    "        'Type 1 Error (FP)': type_1_error,\n",
    "        'Type 2 Error (FN)': type_2_error,\n",
    "        'Confusion Matrix': cm.tolist()  # Convert to list for better readability\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Type 1 Error (FP)</th>\n",
       "      <th>Type 2 Error (FN)</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.771204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>669</td>\n",
       "      <td>[[2255, 0], [669, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.951778</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.843049</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>36</td>\n",
       "      <td>105</td>\n",
       "      <td>[[2219, 36], [105, 564]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.942202</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.934230</td>\n",
       "      <td>0.880902</td>\n",
       "      <td>125</td>\n",
       "      <td>44</td>\n",
       "      <td>[[2130, 125], [44, 625]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.907319</td>\n",
       "      <td>0.799699</td>\n",
       "      <td>0.793722</td>\n",
       "      <td>0.796699</td>\n",
       "      <td>133</td>\n",
       "      <td>138</td>\n",
       "      <td>[[2122, 133], [138, 531]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stacking Model</td>\n",
       "      <td>0.950068</td>\n",
       "      <td>0.913112</td>\n",
       "      <td>0.863976</td>\n",
       "      <td>0.887865</td>\n",
       "      <td>55</td>\n",
       "      <td>91</td>\n",
       "      <td>[[2200, 55], [91, 578]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model  Accuracy  Precision    Recall  F1 Score  \\\n",
       "0       Logistic Regression  0.771204   0.000000  0.000000  0.000000   \n",
       "1             Random Forest  0.951778   0.940000  0.843049  0.888889   \n",
       "2             XGBClassifier  0.942202   0.833333  0.934230  0.880902   \n",
       "3  Decision Tree Classifier  0.907319   0.799699  0.793722  0.796699   \n",
       "4            Stacking Model  0.950068   0.913112  0.863976  0.887865   \n",
       "\n",
       "   Type 1 Error (FP)  Type 2 Error (FN)           Confusion Matrix  \n",
       "0                  0                669      [[2255, 0], [669, 0]]  \n",
       "1                 36                105   [[2219, 36], [105, 564]]  \n",
       "2                125                 44   [[2130, 125], [44, 625]]  \n",
       "3                133                138  [[2122, 133], [138, 531]]  \n",
       "4                 55                 91    [[2200, 55], [91, 578]]  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Analysis and Recommendations\n",
    "\n",
    "1. **Overall Performance**:\n",
    "   - **Random Forest** continues to perform the best overall with the highest precision and F1-scores for both classes.\n",
    "   - **XGBoost** follows closely, demonstrating strong performance, especially in precision for not spam (class 0).\n",
    "   - **Decision Tree** shows reasonable performance but lags behind the other models, particularly in precision for spam (class 1).\n",
    "   - **Logistic Regression** has the lowest metrics overall, particularly in recall for spam (class 1), indicating it misses many spam messages.\n",
    "\n",
    "2. **Precision**:\n",
    "   - **XGBoost** exhibits the highest precision for not spam (0.98) and a competitive score for spam (0.83).\n",
    "   - **Random Forest** has a very high precision for spam (0.94) but lower for not spam compared to XGBoost.\n",
    "   - **Decision Tree** has a good precision for not spam (0.94) but a lower precision for spam (0.80).\n",
    "\n",
    "3. **Recall**:\n",
    "   - **Random Forest** maintains the highest recall for not spam (0.98) and a decent score for spam (0.84).\n",
    "   - **Decision Tree** has good recall for not spam (0.94) but lower recall for spam (0.79).\n",
    "   - **Logistic Regression** has the lowest recall for spam (0.69), which indicates it misses many spam emails.\n",
    "\n",
    "4. **F1-Score**:\n",
    "   - The **Random Forest** achieves the highest F1-scores overall, indicating a good balance between precision and recall for both classes.\n",
    "   - **XGBoost** also has high F1-scores, particularly for not spam (0.96).\n",
    "   - **Decision Tree** has a good F1-score for not spam (0.94) but lower for spam (0.80), while **Logistic Regression** has the lowest F1-scores, especially for spam (0.75).\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Based on the performance metrics:\n",
    "\n",
    "- **Random Forest** is the best model overall for this email spam classification task, showing strong performance in precision, recall, and F1-scores across both classes.\n",
    "- **XGBoost** is a close second, especially notable for its high precision for not spam.\n",
    "- **Decision Tree** performs reasonably well but does not match the other models in terms of spam detection capabilities.\n",
    "- **Logistic Regression** is the weakest performer and may not be suitable for applications where spam detection is critical.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open('random_forest__baseline_model.pkl', 'wb') as file:\n",
    "    pickle.dump(rf_model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
