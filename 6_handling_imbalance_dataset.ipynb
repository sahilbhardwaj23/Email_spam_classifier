{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('training_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>num_stop_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_punctuation_chars</th>\n",
       "      <th>word_count_after_preprocessing</th>\n",
       "      <th>num_chars_after_preprocessing</th>\n",
       "      <th>vector_dim_1</th>\n",
       "      <th>vector_dim_2</th>\n",
       "      <th>vector_dim_3</th>\n",
       "      <th>...</th>\n",
       "      <th>vector_dim_91</th>\n",
       "      <th>vector_dim_92</th>\n",
       "      <th>vector_dim_93</th>\n",
       "      <th>vector_dim_94</th>\n",
       "      <th>vector_dim_95</th>\n",
       "      <th>vector_dim_96</th>\n",
       "      <th>vector_dim_97</th>\n",
       "      <th>vector_dim_98</th>\n",
       "      <th>vector_dim_99</th>\n",
       "      <th>vector_dim_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>111</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>82</td>\n",
       "      <td>-0.445367</td>\n",
       "      <td>-0.101509</td>\n",
       "      <td>-1.050856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363760</td>\n",
       "      <td>0.423471</td>\n",
       "      <td>-0.183125</td>\n",
       "      <td>0.123134</td>\n",
       "      <td>0.421487</td>\n",
       "      <td>-0.085384</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>-0.311313</td>\n",
       "      <td>0.590059</td>\n",
       "      <td>-0.068110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.336330</td>\n",
       "      <td>-0.172123</td>\n",
       "      <td>-0.879673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178105</td>\n",
       "      <td>0.469185</td>\n",
       "      <td>-0.159354</td>\n",
       "      <td>0.181287</td>\n",
       "      <td>0.370253</td>\n",
       "      <td>-0.151212</td>\n",
       "      <td>-0.029242</td>\n",
       "      <td>0.017299</td>\n",
       "      <td>0.309134</td>\n",
       "      <td>-0.183176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>110</td>\n",
       "      <td>-0.234010</td>\n",
       "      <td>-0.005789</td>\n",
       "      <td>-0.754281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179313</td>\n",
       "      <td>0.245597</td>\n",
       "      <td>-0.074740</td>\n",
       "      <td>0.326302</td>\n",
       "      <td>0.246354</td>\n",
       "      <td>0.009656</td>\n",
       "      <td>-0.045136</td>\n",
       "      <td>-0.300550</td>\n",
       "      <td>0.424266</td>\n",
       "      <td>-0.487235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.360958</td>\n",
       "      <td>-0.032422</td>\n",
       "      <td>-1.517050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446681</td>\n",
       "      <td>0.821034</td>\n",
       "      <td>-0.187948</td>\n",
       "      <td>0.338291</td>\n",
       "      <td>0.571384</td>\n",
       "      <td>-0.449642</td>\n",
       "      <td>-0.185700</td>\n",
       "      <td>0.029383</td>\n",
       "      <td>0.535742</td>\n",
       "      <td>-0.069742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>-0.108012</td>\n",
       "      <td>-0.021497</td>\n",
       "      <td>-1.344048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748294</td>\n",
       "      <td>-0.040061</td>\n",
       "      <td>-0.212751</td>\n",
       "      <td>0.148267</td>\n",
       "      <td>-0.139923</td>\n",
       "      <td>-0.006358</td>\n",
       "      <td>0.034125</td>\n",
       "      <td>-0.220026</td>\n",
       "      <td>1.054043</td>\n",
       "      <td>-0.075081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  word_count  num_stop_words  num_chars  num_punctuation_chars  \\\n",
       "0   ham          20               4        111                      9   \n",
       "1   ham           6               0         29                      6   \n",
       "2  spam          28               5        155                      5   \n",
       "3   ham          11               2         49                      6   \n",
       "4   ham          13               6         61                      2   \n",
       "\n",
       "   word_count_after_preprocessing  num_chars_after_preprocessing  \\\n",
       "0                              16                             82   \n",
       "1                               6                             23   \n",
       "2                              20                            110   \n",
       "3                               9                             35   \n",
       "4                               8                             40   \n",
       "\n",
       "   vector_dim_1  vector_dim_2  vector_dim_3  ...  vector_dim_91  \\\n",
       "0     -0.445367     -0.101509     -1.050856  ...       0.363760   \n",
       "1     -0.336330     -0.172123     -0.879673  ...       0.178105   \n",
       "2     -0.234010     -0.005789     -0.754281  ...       0.179313   \n",
       "3     -0.360958     -0.032422     -1.517050  ...       0.446681   \n",
       "4     -0.108012     -0.021497     -1.344048  ...       0.748294   \n",
       "\n",
       "   vector_dim_92  vector_dim_93  vector_dim_94  vector_dim_95  vector_dim_96  \\\n",
       "0       0.423471      -0.183125       0.123134       0.421487      -0.085384   \n",
       "1       0.469185      -0.159354       0.181287       0.370253      -0.151212   \n",
       "2       0.245597      -0.074740       0.326302       0.246354       0.009656   \n",
       "3       0.821034      -0.187948       0.338291       0.571384      -0.449642   \n",
       "4      -0.040061      -0.212751       0.148267      -0.139923      -0.006358   \n",
       "\n",
       "   vector_dim_97  vector_dim_98  vector_dim_99  vector_dim_100  \n",
       "0       0.096360      -0.311313       0.590059       -0.068110  \n",
       "1      -0.029242       0.017299       0.309134       -0.183176  \n",
       "2      -0.045136      -0.300550       0.424266       -0.487235  \n",
       "3      -0.185700       0.029383       0.535742       -0.069742  \n",
       "4       0.034125      -0.220026       1.054043       -0.075081  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "ham     77.23\n",
       "spam    22.77\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts(normalize=True).mul(100).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAGwCAYAAAC0HlECAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAApP0lEQVR4nO3de1hVZaLH8d9G4uJlgxeuIyZlR2VELfUo46VUjpjmGcuaTCY1Ge0CY0jjbcYwrYbC0RTrSNppsDlaao1mWowcTBwVUel4zVuNHp1jGzwp7MQElH3+6LAed1rzRuje6PfzPPt53Ot999rv4nmIb2svFjaXy+USAAAAvpePpxcAAADQEBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAw4OvpBdwoampqdOrUKTVr1kw2m83TywEAAAZcLpe++uorRUZGysfn+88lEU315NSpU4qKivL0MgAAQB2cPHlSrVu3/t45RFM9adasmaRvvuh2u93DqwEAACacTqeioqKsn+Pfh2iqJ7UfydntdqIJAIAGxuTSGi4EBwAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMCAr6cXgB+m2+S3PL0EwOsUzxnt6SUAuAlwpgkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMODRaNq8ebOGDRumyMhI2Ww2rVmzxm3c5XIpPT1dERERCgwMVHx8vI4ePeo258yZM0pMTJTdbldwcLCSkpJ07tw5tzl79+5V3759FRAQoKioKGVmZl6xllWrVqlDhw4KCAhQbGysPvzww3o/XgAA0HB5NJoqKirUpUsXvfbaa1cdz8zMVFZWlrKzs1VUVKQmTZooISFBFy5csOYkJibqwIEDysvL07p167R582ZNmDDBGnc6nRo0aJBuvfVWFRcXa86cOXruuee0ePFia862bdv0yCOPKCkpSf/1X/+l4cOHa/jw4dq/f/+1O3gAANCg2Fwul8vTi5Akm82m1atXa/jw4ZK+OcsUGRmpZ555Rr/5zW8kSeXl5QoLC1NOTo5GjhypgwcPKiYmRjt37lT37t0lSbm5uRoyZIj+/ve/KzIyUosWLdLvfvc7ORwO+fn5SZKmTZumNWvW6NChQ5Kkhx9+WBUVFVq3bp21nl69eqlr167Kzs42Wr/T6VRQUJDKy8tlt9vr68tyhW6T37pm+wYaquI5oz29BAAN1A/5+e211zQdO3ZMDodD8fHx1ragoCD17NlThYWFkqTCwkIFBwdbwSRJ8fHx8vHxUVFRkTWnX79+VjBJUkJCgg4fPqyzZ89acy5/n9o5te9zNZWVlXI6nW4PAABw4/LaaHI4HJKksLAwt+1hYWHWmMPhUGhoqNu4r6+vWrRo4Tbnavu4/D2+a07t+NVkZGQoKCjIekRFRf3QQwQAAA2I10aTt5s+fbrKy8utx8mTJz29JAAAcA15bTSFh4dLkkpKSty2l5SUWGPh4eEqLS11G7948aLOnDnjNudq+7j8Pb5rTu341fj7+8tut7s9AADAjctroyk6Olrh4eHKz8+3tjmdThUVFSkuLk6SFBcXp7KyMhUXF1tzNm7cqJqaGvXs2dOas3nzZlVXV1tz8vLy1L59ezVv3tyac/n71M6pfR8AAACPRtO5c+e0e/du7d69W9I3F3/v3r1bJ06ckM1mU2pqql544QWtXbtW+/bt0+jRoxUZGWn9hl3Hjh01ePBgjR8/Xjt27NDWrVuVkpKikSNHKjIyUpI0atQo+fn5KSkpSQcOHNCKFSu0YMECpaWlWet4+umnlZubq7lz5+rQoUN67rnntGvXLqWkpFzvLwkAAPBSvp588127dql///7W89qQGTNmjHJycjRlyhRVVFRowoQJKisrU58+fZSbm6uAgADrNcuWLVNKSooGDhwoHx8fjRgxQllZWdZ4UFCQNmzYoOTkZHXr1k2tWrVSenq6272cfvazn2n58uWaMWOGfvvb3+qOO+7QmjVr1KlTp+vwVQAAAA2B19ynqaHjPk2A53CfJgB1dUPcpwkAAMCbEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABr46mS5cu6dlnn1V0dLQCAwN1++236/nnn5fL5bLmuFwupaenKyIiQoGBgYqPj9fRo0fd9nPmzBklJibKbrcrODhYSUlJOnfunNucvXv3qm/fvgoICFBUVJQyMzOvyzECAICGwauj6eWXX9aiRYv06quv6uDBg3r55ZeVmZmphQsXWnMyMzOVlZWl7OxsFRUVqUmTJkpISNCFCxesOYmJiTpw4IDy8vK0bt06bd68WRMmTLDGnU6nBg0apFtvvVXFxcWaM2eOnnvuOS1evPi6Hi8AAPBevp5ewPfZtm2bfv7zn2vo0KGSpLZt2+rtt9/Wjh07JH1zlmn+/PmaMWOGfv7zn0uS3nrrLYWFhWnNmjUaOXKkDh48qNzcXO3cuVPdu3eXJC1cuFBDhgzRH/7wB0VGRmrZsmWqqqrSm2++KT8/P/30pz/V7t27NW/ePLe4ulxlZaUqKyut506n81p+KQAAgId59Zmmn/3sZ8rPz9eRI0ckSXv27NGWLVt07733SpKOHTsmh8Oh+Ph46zVBQUHq2bOnCgsLJUmFhYUKDg62gkmS4uPj5ePjo6KiImtOv3795OfnZ81JSEjQ4cOHdfbs2auuLSMjQ0FBQdYjKiqqfg8eAAB4Fa8+0zRt2jQ5nU516NBBjRo10qVLl/Tiiy8qMTFRkuRwOCRJYWFhbq8LCwuzxhwOh0JDQ93GfX191aJFC7c50dHRV+yjdqx58+ZXrG369OlKS0uznjudTsIJAIAbmFdH08qVK7Vs2TItX77c+sgsNTVVkZGRGjNmjEfX5u/vL39/f4+uAQAAXD9eHU2TJ0/WtGnTNHLkSElSbGys/vu//1sZGRkaM2aMwsPDJUklJSWKiIiwXldSUqKuXbtKksLDw1VaWuq234sXL+rMmTPW68PDw1VSUuI2p/Z57RwAAHBz8+prms6fPy8fH/clNmrUSDU1NZKk6OhohYeHKz8/3xp3Op0qKipSXFycJCkuLk5lZWUqLi625mzcuFE1NTXq2bOnNWfz5s2qrq625uTl5al9+/ZX/WgOAADcfLw6moYNG6YXX3xR69ev1/Hjx7V69WrNmzdP999/vyTJZrMpNTVVL7zwgtauXat9+/Zp9OjRioyM1PDhwyVJHTt21ODBgzV+/Hjt2LFDW7duVUpKikaOHKnIyEhJ0qhRo+Tn56ekpCQdOHBAK1as0IIFC9yuWQIAADc3r/54buHChXr22Wf11FNPqbS0VJGRkXr88ceVnp5uzZkyZYoqKio0YcIElZWVqU+fPsrNzVVAQIA1Z9myZUpJSdHAgQPl4+OjESNGKCsryxoPCgrShg0blJycrG7duqlVq1ZKT0//ztsNAACAm4/NdfnttVFnTqdTQUFBKi8vl91uv2bv023yW9ds30BDVTxntKeXAKCB+iE/v7364zkAAABvQTQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYKBO0TRgwACVlZVdsd3pdGrAgAE/dk0AAABep07RtGnTJlVVVV2x/cKFC/rrX//6oxcFAADgbXx/yOS9e/da//7000/lcDis55cuXVJubq5+8pOf1N/qAAAAvMQPiqauXbvKZrPJZrNd9WO4wMBALVy4sN4WBwAA4C1+UDQdO3ZMLpdLt912m3bs2KGQkBBrzM/PT6GhoWrUqFG9LxIAAMDTflA03XrrrZKkmpqaa7IYAAAAb/WDoulyR48e1ccff6zS0tIrIio9Pf1HLwwAAMCb1CmalixZoieffFKtWrVSeHi4bDabNWaz2YgmAABww6lTNL3wwgt68cUXNXXq1PpeDwAAgFeq032azp49q4ceeqi+1wIAAOC16hRNDz30kDZs2FDfawEAAPBadYqmdu3a6dlnn9XYsWM1d+5cZWVluT3q0//8z//ol7/8pVq2bKnAwEDFxsZq165d1rjL5VJ6eroiIiIUGBio+Ph4HT161G0fZ86cUWJioux2u4KDg5WUlKRz5865zdm7d6/69u2rgIAARUVFKTMzs16PAwAANGx1uqZp8eLFatq0qQoKClRQUOA2ZrPZNHHixHpZ3NmzZ9W7d2/1799fH330kUJCQnT06FE1b97cmpOZmamsrCwtXbpU0dHRevbZZ5WQkKBPP/1UAQEBkqTExER98cUXysvLU3V1tR577DFNmDBBy5cvl/TN38wbNGiQ4uPjlZ2drX379mncuHEKDg7WhAkT6uVYAABAw2ZzuVwuTy/iu0ybNk1bt279zr9n53K5FBkZqWeeeUa/+c1vJEnl5eUKCwtTTk6ORo4cqYMHDyomJkY7d+5U9+7dJUm5ubkaMmSI/v73vysyMlKLFi3S7373OzkcDvn5+VnvvWbNGh06dMhorU6nU0FBQSovL5fdbq+Ho7+6bpPfumb7Bhqq4jmjPb0EAA3UD/n5XaeP566XtWvXqnv37nrooYcUGhqqO++8U0uWLLHGjx07JofDofj4eGtbUFCQevbsqcLCQklSYWGhgoODrWCSpPj4ePn4+KioqMia069fPyuYJCkhIUGHDx/W2bNnr7q2yspKOZ1OtwcAALhx1enjuXHjxn3v+JtvvlmnxXzb3/72Ny1atEhpaWn67W9/q507d2rixIny8/PTmDFjrD8YHBYW5va6sLAwa8zhcCg0NNRt3NfXVy1atHCbEx0dfcU+ascu/ziwVkZGhmbNmlUvxwkAALxfnaLp22dfqqurtX//fpWVlV31D/nWVU1Njbp3767f//73kqQ777xT+/fvV3Z2tsaMGVNv71MX06dPV1pamvXc6XQqKirKgysCAADXUp2iafXq1Vdsq6mp0ZNPPqnbb7/9Ry+qVkREhGJiYty2dezYUe+9954kKTw8XJJUUlKiiIgIa05JSYm6du1qzSktLXXbx8WLF3XmzBnr9eHh4SopKXGbU/u8ds63+fv7y9/fv45HBgAAGpp6u6bJx8dHaWlpeuWVV+prl+rdu7cOHz7stu3IkSPWHw6Ojo5WeHi48vPzrXGn06mioiLFxcVJkuLi4lRWVqbi4mJrzsaNG1VTU6OePXtaczZv3qzq6mprTl5entq3b3/Vj+YAAMDNp14vBP/888918eLFetvfpEmTtH37dv3+97/XZ599puXLl2vx4sVKTk6W9M3tDVJTU/XCCy9o7dq12rdvn0aPHq3IyEgNHz5c0jdnpgYPHqzx48drx44d2rp1q1JSUjRy5EhFRkZKkkaNGiU/Pz8lJSXpwIEDWrFihRYsWOD28RsAALi51enjuW/HhMvl0hdffKH169fX67VGPXr00OrVqzV9+nTNnj1b0dHRmj9/vhITE605U6ZMUUVFhSZMmKCysjL16dNHubm51j2aJGnZsmVKSUnRwIED5ePjoxEjRrjdhDMoKEgbNmxQcnKyunXrplatWik9PZ17NAEAAEud7tPUv39/t+c+Pj4KCQnRgAEDNG7cOPn61qnFGjTu0wR4DvdpAlBXP+Tnd53q5uOPP67TwgAAABqqH3VK6PTp09aF2u3bt1dISEi9LAoAAMDb1OlC8IqKCo0bN04RERHq16+f+vXrp8jISCUlJen8+fP1vUYAAACPq1M0paWlqaCgQB988IHKyspUVlam999/XwUFBXrmmWfqe40AAAAeV6eP59577z29++67uueee6xtQ4YMUWBgoH7xi19o0aJF9bU+AAAAr1CnM03nz5+/4u+9SVJoaCgfzwEAgBtSnaIpLi5OM2fO1IULF6xtX3/9tWbNmmXdiRsAAOBGUqeP5+bPn6/BgwerdevW6tKliyRpz5498vf314YNG+p1gQAAAN6gTtEUGxuro0ePatmyZTp06JAk6ZFHHlFiYqICAwPrdYEAAADeoE7RlJGRobCwMI0fP95t+5tvvqnTp09r6tSp9bI4AAAAb1Gna5pef/11dejQ4YrtP/3pT5Wdnf2jFwUAAOBt6hRNDodDERERV2wPCQnRF1988aMXBQAA4G3qFE1RUVHaunXrFdu3bt2qyMjIH70oAAAAb1Ona5rGjx+v1NRUVVdXa8CAAZKk/Px8TZkyhTuCAwCAG1Kdomny5Mn68ssv9dRTT6mqqkqSFBAQoKlTp2r69On1ukAAAABvUKdostlsevnll/Xss8/q4MGDCgwM1B133CF/f//6Xh8AAIBXqFM01WratKl69OhRX2sBAADwWnW6EBwAAOBmQzQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABhoUNH00ksvyWazKTU11dp24cIFJScnq2XLlmratKlGjBihkpISt9edOHFCQ4cOVePGjRUaGqrJkyfr4sWLbnM2bdqku+66S/7+/mrXrp1ycnKuwxEBAICGosFE086dO/X666+rc+fObtsnTZqkDz74QKtWrVJBQYFOnTqlBx54wBq/dOmShg4dqqqqKm3btk1Lly5VTk6O0tPTrTnHjh3T0KFD1b9/f+3evVupqan61a9+pb/85S/X7fgAAIB3axDRdO7cOSUmJmrJkiVq3ry5tb28vFz//u//rnnz5mnAgAHq1q2b/vjHP2rbtm3avn27JGnDhg369NNP9R//8R/q2rWr7r33Xj3//PN67bXXVFVVJUnKzs5WdHS05s6dq44dOyolJUUPPvigXnnlFY8cLwAA8D4NIpqSk5M1dOhQxcfHu20vLi5WdXW12/YOHTqoTZs2KiwslCQVFhYqNjZWYWFh1pyEhAQ5nU4dOHDAmvPtfSckJFj7uJrKyko5nU63BwAAuHH5enoB/8g777yjTz75RDt37rxizOFwyM/PT8HBwW7bw8LC5HA4rDmXB1PteO3Y981xOp36+uuvFRgYeMV7Z2RkaNasWXU+LgAA0LB49ZmmkydP6umnn9ayZcsUEBDg6eW4mT59usrLy63HyZMnPb0kAABwDXl1NBUXF6u0tFR33XWXfH195evrq4KCAmVlZcnX11dhYWGqqqpSWVmZ2+tKSkoUHh4uSQoPD7/it+lqn/+jOXa7/apnmSTJ399fdrvd7QEAAG5cXh1NAwcO1L59+7R7927r0b17dyUmJlr/vuWWW5Sfn2+95vDhwzpx4oTi4uIkSXFxcdq3b59KS0utOXl5ebLb7YqJibHmXL6P2jm1+wAAAPDqa5qaNWumTp06uW1r0qSJWrZsaW1PSkpSWlqaWrRoIbvdrl//+teKi4tTr169JEmDBg1STEyMHn30UWVmZsrhcGjGjBlKTk6Wv7+/JOmJJ57Qq6++qilTpmjcuHHauHGjVq5cqfXr11/fAwYAAF7Lq6PJxCuvvCIfHx+NGDFClZWVSkhI0L/9279Z440aNdK6dev05JNPKi4uTk2aNNGYMWM0e/Zsa050dLTWr1+vSZMmacGCBWrdurXeeOMNJSQkeOKQAACAF7K5XC6XpxdxI3A6nQoKClJ5efk1vb6p2+S3rtm+gYaqeM5oTy8BQAP1Q35+e/U1TQAAAN6CaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA76eXgAA4BsnZsd6egmA12mTvs/TS7BwpgkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADHh1NGVkZKhHjx5q1qyZQkNDNXz4cB0+fNhtzoULF5ScnKyWLVuqadOmGjFihEpKStzmnDhxQkOHDlXjxo0VGhqqyZMn6+LFi25zNm3apLvuukv+/v5q166dcnJyrvXhAQCABsSro6mgoEDJycnavn278vLyVF1drUGDBqmiosKaM2nSJH3wwQdatWqVCgoKdOrUKT3wwAPW+KVLlzR06FBVVVVp27ZtWrp0qXJycpSenm7NOXbsmIYOHar+/ftr9+7dSk1N1a9+9Sv95S9/ua7HCwAAvJfN5XK5PL0IU6dPn1ZoaKgKCgrUr18/lZeXKyQkRMuXL9eDDz4oSTp06JA6duyowsJC9erVSx999JHuu+8+nTp1SmFhYZKk7OxsTZ06VadPn5afn5+mTp2q9evXa//+/dZ7jRw5UmVlZcrNzb3qWiorK1VZWWk9dzqdioqKUnl5uex2+zX7GnSb/NY12zfQUBXPGe3pJdSLE7NjPb0EwOu0Sd93TffvdDoVFBRk9PPbq880fVt5ebkkqUWLFpKk4uJiVVdXKz4+3prToUMHtWnTRoWFhZKkwsJCxcbGWsEkSQkJCXI6nTpw4IA15/J91M6p3cfVZGRkKCgoyHpERUXVz0ECAACv1GCiqaamRqmpqerdu7c6deokSXI4HPLz81NwcLDb3LCwMDkcDmvO5cFUO1479n1znE6nvv7666uuZ/r06SovL7ceJ0+e/NHHCAAAvJevpxdgKjk5Wfv379eWLVs8vRRJkr+/v/z9/T29DAAAcJ00iDNNKSkpWrdunT7++GO1bt3a2h4eHq6qqiqVlZW5zS8pKVF4eLg159u/TVf7/B/NsdvtCgwMrO/DAQAADZBXR5PL5VJKSopWr16tjRs3Kjo62m28W7duuuWWW5Sfn29tO3z4sE6cOKG4uDhJUlxcnPbt26fS0lJrTl5enux2u2JiYqw5l++jdk7tPgAAALz647nk5GQtX75c77//vpo1a2ZdgxQUFKTAwEAFBQUpKSlJaWlpatGihex2u379618rLi5OvXr1kiQNGjRIMTExevTRR5WZmSmHw6EZM2YoOTnZ+njtiSee0KuvvqopU6Zo3Lhx2rhxo1auXKn169d77NgBAIB38eozTYsWLVJ5ebnuueceRUREWI8VK1ZYc1555RXdd999GjFihPr166fw8HD9+c9/tsYbNWqkdevWqVGjRoqLi9Mvf/lLjR49WrNnz7bmREdHa/369crLy1OXLl00d+5cvfHGG0pISLiuxwsAALxXg7pPkzf7Ifd5+DG4TxNwJe7TBNy4uE8TAABAA0M0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBNAAAABogmAAAAA0QTAACAAaIJAADAANEEAABggGgCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBMAAIABogkAAMAA0QQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGCCaAAAADBBN3/Laa6+pbdu2CggIUM+ePbVjxw5PLwkAAHgBoukyK1asUFpammbOnKlPPvlEXbp0UUJCgkpLSz29NAAA4GFE02XmzZun8ePH67HHHlNMTIyys7PVuHFjvfnmm55eGgAA8DBfTy/AW1RVVam4uFjTp0+3tvn4+Cg+Pl6FhYVXzK+srFRlZaX1vLy8XJLkdDqv6TovVX59TfcPNETX+vvuevnqwiVPLwHwOtf6+7t2/y6X6x/OJZr+3//+7//q0qVLCgsLc9seFhamQ4cOXTE/IyNDs2bNumJ7VFTUNVsjgKsLWviEp5cA4FrJCLoub/PVV18pKOj734toqqPp06crLS3Nel5TU6MzZ86oZcuWstlsHlwZrgen06moqCidPHlSdrvd08sBUI/4/r65uFwuffXVV4qMjPyHc4mm/9eqVSs1atRIJSUlbttLSkoUHh5+xXx/f3/5+/u7bQsODr6WS4QXstvt/EcVuEHx/X3z+EdnmGpxIfj/8/PzU7du3ZSfn29tq6mpUX5+vuLi4jy4MgAA4A0403SZtLQ0jRkzRt27d9c///M/a/78+aqoqNBjjz3m6aUBAAAPI5ou8/DDD+v06dNKT0+Xw+FQ165dlZube8XF4YC/v79mzpx5xUe0ABo+vr/xXWwuk9+xAwAAuMlxTRMAAIABogkAAMAA0QQAAGCAaMJN75577lFqaqqnlwEA8HJEEwAAgAGiCQAAwADRBOibu79PmTJFLVq0UHh4uJ577jlrbN68eYqNjVWTJk0UFRWlp556SufOnbPGc3JyFBwcrHXr1ql9+/Zq3LixHnzwQZ0/f15Lly5V27Zt1bx5c02cOFGXLvFX7IFr7d1331VsbKwCAwPVsmVLxcfHq6KiQmPHjtXw4cM1a9YshYSEyG6364knnlBVVZX12tzcXPXp00fBwcFq2bKl7rvvPn3++efW+PHjx2Wz2bRy5Ur17dtXgYGB6tGjh44cOaKdO3eqe/fuatq0qe69916dPn3aE4ePa4hoAiQtXbpUTZo0UVFRkTIzMzV79mzl5eVJknx8fJSVlaUDBw5o6dKl2rhxo6ZMmeL2+vPnzysrK0vvvPOOcnNztWnTJt1///368MMP9eGHH+pPf/qTXn/9db377rueODzgpvHFF1/okUce0bhx43Tw4EFt2rRJDzzwgGpvSZifn29tf/vtt/XnP/9Zs2bNsl5fUVGhtLQ07dq1S/n5+fLx8dH999+vmpoat/eZOXOmZsyYoU8++US+vr4aNWqUpkyZogULFuivf/2rPvvsM6Wnp1/XY8d14AJucnfffberT58+btt69Ojhmjp16lXnr1q1ytWyZUvr+R//+EeXJNdnn31mbXv88cddjRs3dn311VfWtoSEBNfjjz9ez6sHcLni4mKXJNfx48evGBszZoyrRYsWroqKCmvbokWLXE2bNnVdunTpqvs7ffq0S5Jr3759LpfL5Tp27JhLkuuNN96w5rz99tsuSa78/HxrW0ZGhqt9+/b1dVjwEpxpAiR17tzZ7XlERIRKS0slSf/5n/+pgQMH6ic/+YmaNWumRx99VF9++aXOnz9vzW/cuLFuv/1263lYWJjatm2rpk2bum2r3SeAa6NLly4aOHCgYmNj9dBDD2nJkiU6e/as23jjxo2t53FxcTp37pxOnjwpSTp69KgeeeQR3XbbbbLb7Wrbtq0k6cSJE27vc/l/M2r/1FZsbKzbNr7fbzxEEyDplltucXtus9lUU1Oj48eP67777lPnzp313nvvqbi4WK+99pokuV0HcbXXf9c+AVw7jRo1Ul5enj766CPFxMRo4cKFat++vY4dO2b0+mHDhunMmTNasmSJioqKVFRUJMn9+11y/5632WxX3cb3+42HP9gLfI/i4mLV1NRo7ty58vH55v8xVq5c6eFVAfg+NptNvXv3Vu/evZWenq5bb71Vq1evliTt2bNHX3/9tQIDAyVJ27dvV9OmTRUVFaUvv/xShw8f1pIlS9S3b19J0pYtWzx2HPA+RBPwPdq1a6fq6motXLhQw4YN09atW5Wdne3pZQH4DkVFRcrPz9egQYMUGhqqoqIinT59Wh07dtTevXtVVVWlpKQkzZgxQ8ePH9fMmTOVkpIiHx8fNW/eXC1bttTixYsVERGhEydOaNq0aZ4+JHgRPp4DvkeXLl00b948vfzyy+rUqZOWLVumjIwMTy8LwHew2+3avHmzhgwZon/6p3/SjBkzNHfuXN17772SpIEDB+qOO+5Qv3799PDDD+tf//VfrVuM+Pj46J133lFxcbE6deqkSZMmac6cOR48Gngbm8v1/7+HCQDADWzs2LEqKyvTmjVrPL0UNFCcaQIAADBANAEAABjg4zkAAAADnGkCAAAwQDQBAAAYIJoAAAAMEE0AAAAGiCYAAAADRBOAm8Y999yj1NRUo7mbNm2SzWZTWVnZj3rPtm3bav78+T9qHwC8A9EEAABggGgCAAAwQDQBuCn96U9/Uvfu3dWsWTOFh4dr1KhRKi0tvWLe1q1b1blzZwUEBKhXr17av3+/2/iWLVvUt29fBQYGKioqShMnTlRFRcX1OgwA1xHRBOCmVF1dreeff1579uzRmjVrdPz4cY0dO/aKeZMnT9bcuXO1c+dOhYSEaNiwYaqurpYkff755xo8eLBGjBihvXv3asWKFdqyZYtSUlKu89EAuB58Pb0AAPCEcePGWf++7bbblJWVpR49eujcuXNq2rSpNTZz5kz9y7/8iyRp6dKlat26tVavXq1f/OIXysjIUGJionVx+R133KGsrCzdfffdWrRokQICAq7rMQG4tjjTBOCmVFxcrGHDhqlNmzZq1qyZ7r77bknSiRMn3ObFxcVZ/27RooXat2+vgwcPSpL27NmjnJwcNW3a1HokJCSopqZGx44du34HA+C64EwTgJtORUWFEhISlJCQoGXLlikkJEQnTpxQQkKCqqqqjPdz7tw5Pf7445o4ceIVY23atKnPJQPwAkQTgJvOoUOH9OWXX+qll15SVFSUJGnXrl1Xnbt9+3YrgM6ePasjR46oY8eOkqS77rpLn376qdq1a3d9Fg7Ao/h4DsBNp02bNvLz89PChQv1t7/9TWvXrtXzzz9/1bmzZ89Wfn6+9u/fr7Fjx6pVq1YaPny4JGnq1Knatm2bUlJStHv3bh09elTvv/8+F4IDNyiiCcBNJyQkRDk5OVq1apViYmL00ksv6Q9/+MNV57700kt6+umn1a1bNzkcDn3wwQfy8/OTJHXu3FkFBQU6cuSI+vbtqzvvvFPp6emKjIy8nocD4DqxuVwul6cXAQAA4O040wQAAGCAaAIAADBANAEAABggmgAAAAwQTQAAAAaIJgAAAANEEwAAgAGiCQAAwADRBAAAYIBoAgAAMEA0AQAAGPg/kQFmc48kFwsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=df,x='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will try different technique to handle this imbalance dataset and then train logistic regression model to improve our confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under sampling is a technique used to balance imbalanced datasets. When a dataset has \n",
    "unequal distribution of classes, meaning one class (the majority class) has more samples and \n",
    "another class (the minority class) has fewer samples, under sampling can be used to address \n",
    "imbalanced data issues.\n",
    "\n",
    "The concept of under sampling is straightforward: you randomly remove some samples from \n",
    "the majority class to create a balance between the classes. The objective is to reduce the size \n",
    "of the majority class to bring it closer to the minority class, allowing machine learning \n",
    "algorithms to work with balanced training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into majority and minority classes\n",
    "majority_df = df[df['label'] == 'ham']\n",
    "minority_df = df[df['label'] == 'spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Apply random undersampling to the majority class\n",
    "undersampled_majority_df = majority_df.sample(n=minority_df.shape[0], random_state=42)\n",
    "\n",
    "# Combine the undersampled majority class with the minority class\n",
    "undersampled_df = pd.concat([undersampled_majority_df, minority_df])\n",
    "\n",
    "# Shuffle the resulting DataFrame to avoid any order bias\n",
    "undersampled_df = shuffle(undersampled_df, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>word_count</th>\n",
       "      <th>num_stop_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_punctuation_chars</th>\n",
       "      <th>word_count_after_preprocessing</th>\n",
       "      <th>num_chars_after_preprocessing</th>\n",
       "      <th>vector_dim_1</th>\n",
       "      <th>vector_dim_2</th>\n",
       "      <th>vector_dim_3</th>\n",
       "      <th>...</th>\n",
       "      <th>vector_dim_91</th>\n",
       "      <th>vector_dim_92</th>\n",
       "      <th>vector_dim_93</th>\n",
       "      <th>vector_dim_94</th>\n",
       "      <th>vector_dim_95</th>\n",
       "      <th>vector_dim_96</th>\n",
       "      <th>vector_dim_97</th>\n",
       "      <th>vector_dim_98</th>\n",
       "      <th>vector_dim_99</th>\n",
       "      <th>vector_dim_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9454</th>\n",
       "      <td>spam</td>\n",
       "      <td>478</td>\n",
       "      <td>111</td>\n",
       "      <td>2329</td>\n",
       "      <td>94</td>\n",
       "      <td>225</td>\n",
       "      <td>1556</td>\n",
       "      <td>-0.778515</td>\n",
       "      <td>-0.034710</td>\n",
       "      <td>-1.238543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220476</td>\n",
       "      <td>0.550449</td>\n",
       "      <td>-0.196538</td>\n",
       "      <td>0.359155</td>\n",
       "      <td>0.893357</td>\n",
       "      <td>0.095854</td>\n",
       "      <td>0.149918</td>\n",
       "      <td>-0.342361</td>\n",
       "      <td>0.575483</td>\n",
       "      <td>-0.633315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>ham</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.765793</td>\n",
       "      <td>-0.064920</td>\n",
       "      <td>-1.341933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519094</td>\n",
       "      <td>0.500147</td>\n",
       "      <td>0.022529</td>\n",
       "      <td>0.176661</td>\n",
       "      <td>0.185421</td>\n",
       "      <td>-0.198955</td>\n",
       "      <td>0.122405</td>\n",
       "      <td>-0.329255</td>\n",
       "      <td>0.883244</td>\n",
       "      <td>-0.366795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7825</th>\n",
       "      <td>spam</td>\n",
       "      <td>94</td>\n",
       "      <td>11</td>\n",
       "      <td>424</td>\n",
       "      <td>27</td>\n",
       "      <td>50</td>\n",
       "      <td>302</td>\n",
       "      <td>-0.373608</td>\n",
       "      <td>-0.115300</td>\n",
       "      <td>-1.290435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518768</td>\n",
       "      <td>0.173426</td>\n",
       "      <td>0.061016</td>\n",
       "      <td>0.393073</td>\n",
       "      <td>0.168801</td>\n",
       "      <td>-0.035846</td>\n",
       "      <td>-0.023936</td>\n",
       "      <td>-0.273413</td>\n",
       "      <td>0.810894</td>\n",
       "      <td>-0.339501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>ham</td>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>92</td>\n",
       "      <td>-0.496377</td>\n",
       "      <td>-0.416542</td>\n",
       "      <td>-1.436387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330355</td>\n",
       "      <td>0.496844</td>\n",
       "      <td>-0.158790</td>\n",
       "      <td>0.455779</td>\n",
       "      <td>0.295092</td>\n",
       "      <td>-0.023337</td>\n",
       "      <td>0.315523</td>\n",
       "      <td>-0.158625</td>\n",
       "      <td>0.811924</td>\n",
       "      <td>-0.361757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>ham</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.257812</td>\n",
       "      <td>-0.004569</td>\n",
       "      <td>-0.342380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222268</td>\n",
       "      <td>0.197743</td>\n",
       "      <td>-0.013578</td>\n",
       "      <td>0.007772</td>\n",
       "      <td>0.123522</td>\n",
       "      <td>-0.029257</td>\n",
       "      <td>-0.026652</td>\n",
       "      <td>-0.176841</td>\n",
       "      <td>0.189215</td>\n",
       "      <td>-0.138940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>spam</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>177</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>124</td>\n",
       "      <td>-0.355055</td>\n",
       "      <td>-0.042373</td>\n",
       "      <td>-0.934360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377327</td>\n",
       "      <td>0.277465</td>\n",
       "      <td>0.046666</td>\n",
       "      <td>0.097335</td>\n",
       "      <td>0.166501</td>\n",
       "      <td>-0.058843</td>\n",
       "      <td>-0.003134</td>\n",
       "      <td>-0.397526</td>\n",
       "      <td>0.589346</td>\n",
       "      <td>-0.214470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9411</th>\n",
       "      <td>spam</td>\n",
       "      <td>43</td>\n",
       "      <td>12</td>\n",
       "      <td>218</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>145</td>\n",
       "      <td>-0.397201</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>-0.906342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415713</td>\n",
       "      <td>0.105170</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.094557</td>\n",
       "      <td>0.160885</td>\n",
       "      <td>-0.023053</td>\n",
       "      <td>-0.002466</td>\n",
       "      <td>-0.444291</td>\n",
       "      <td>0.633426</td>\n",
       "      <td>-0.217753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9519</th>\n",
       "      <td>spam</td>\n",
       "      <td>103</td>\n",
       "      <td>22</td>\n",
       "      <td>557</td>\n",
       "      <td>24</td>\n",
       "      <td>54</td>\n",
       "      <td>397</td>\n",
       "      <td>-0.430365</td>\n",
       "      <td>-0.035594</td>\n",
       "      <td>-0.832599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540451</td>\n",
       "      <td>0.257317</td>\n",
       "      <td>-0.164706</td>\n",
       "      <td>0.015197</td>\n",
       "      <td>0.215511</td>\n",
       "      <td>-0.022273</td>\n",
       "      <td>-0.037960</td>\n",
       "      <td>-0.374827</td>\n",
       "      <td>0.669957</td>\n",
       "      <td>-0.326614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>spam</td>\n",
       "      <td>101</td>\n",
       "      <td>30</td>\n",
       "      <td>536</td>\n",
       "      <td>19</td>\n",
       "      <td>52</td>\n",
       "      <td>372</td>\n",
       "      <td>-0.328155</td>\n",
       "      <td>-0.081961</td>\n",
       "      <td>-1.136878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568055</td>\n",
       "      <td>0.193118</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>0.192594</td>\n",
       "      <td>0.087289</td>\n",
       "      <td>-0.011131</td>\n",
       "      <td>-0.038508</td>\n",
       "      <td>-0.428401</td>\n",
       "      <td>0.821097</td>\n",
       "      <td>-0.333147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4678</th>\n",
       "      <td>ham</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.201455</td>\n",
       "      <td>-0.253204</td>\n",
       "      <td>-1.155382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461047</td>\n",
       "      <td>-0.263939</td>\n",
       "      <td>-0.138482</td>\n",
       "      <td>0.153497</td>\n",
       "      <td>0.241605</td>\n",
       "      <td>0.235931</td>\n",
       "      <td>0.133260</td>\n",
       "      <td>-0.193747</td>\n",
       "      <td>0.769481</td>\n",
       "      <td>-0.100121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6658 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  word_count  num_stop_words  num_chars  num_punctuation_chars  \\\n",
       "9454   spam         478             111       2329                     94   \n",
       "539     ham           3               0         22                      4   \n",
       "7825   spam          94              11        424                     27   \n",
       "3874    ham          36              16        147                      2   \n",
       "2351    ham           5               0         25                      2   \n",
       "...     ...         ...             ...        ...                    ...   \n",
       "3464   spam          27               4        177                      7   \n",
       "9411   spam          43              12        218                      7   \n",
       "9519   spam         103              22        557                     24   \n",
       "10011  spam         101              30        536                     19   \n",
       "4678    ham          12               7         44                      0   \n",
       "\n",
       "       word_count_after_preprocessing  num_chars_after_preprocessing  \\\n",
       "9454                              225                           1556   \n",
       "539                                 3                             13   \n",
       "7825                               50                            302   \n",
       "3874                               20                             92   \n",
       "2351                                5                             23   \n",
       "...                               ...                            ...   \n",
       "3464                               18                            124   \n",
       "9411                               22                            145   \n",
       "9519                               54                            397   \n",
       "10011                              52                            372   \n",
       "4678                                5                             24   \n",
       "\n",
       "       vector_dim_1  vector_dim_2  vector_dim_3  ...  vector_dim_91  \\\n",
       "9454      -0.778515     -0.034710     -1.238543  ...       0.220476   \n",
       "539       -0.765793     -0.064920     -1.341933  ...       0.519094   \n",
       "7825      -0.373608     -0.115300     -1.290435  ...       0.518768   \n",
       "3874      -0.496377     -0.416542     -1.436387  ...       0.330355   \n",
       "2351      -0.257812     -0.004569     -0.342380  ...       0.222268   \n",
       "...             ...           ...           ...  ...            ...   \n",
       "3464      -0.355055     -0.042373     -0.934360  ...       0.377327   \n",
       "9411      -0.397201      0.006813     -0.906342  ...       0.415713   \n",
       "9519      -0.430365     -0.035594     -0.832599  ...       0.540451   \n",
       "10011     -0.328155     -0.081961     -1.136878  ...       0.568055   \n",
       "4678      -0.201455     -0.253204     -1.155382  ...       0.461047   \n",
       "\n",
       "       vector_dim_92  vector_dim_93  vector_dim_94  vector_dim_95  \\\n",
       "9454        0.550449      -0.196538       0.359155       0.893357   \n",
       "539         0.500147       0.022529       0.176661       0.185421   \n",
       "7825        0.173426       0.061016       0.393073       0.168801   \n",
       "3874        0.496844      -0.158790       0.455779       0.295092   \n",
       "2351        0.197743      -0.013578       0.007772       0.123522   \n",
       "...              ...            ...            ...            ...   \n",
       "3464        0.277465       0.046666       0.097335       0.166501   \n",
       "9411        0.105170       0.004165       0.094557       0.160885   \n",
       "9519        0.257317      -0.164706       0.015197       0.215511   \n",
       "10011       0.193118      -0.091383       0.192594       0.087289   \n",
       "4678       -0.263939      -0.138482       0.153497       0.241605   \n",
       "\n",
       "       vector_dim_96  vector_dim_97  vector_dim_98  vector_dim_99  \\\n",
       "9454        0.095854       0.149918      -0.342361       0.575483   \n",
       "539        -0.198955       0.122405      -0.329255       0.883244   \n",
       "7825       -0.035846      -0.023936      -0.273413       0.810894   \n",
       "3874       -0.023337       0.315523      -0.158625       0.811924   \n",
       "2351       -0.029257      -0.026652      -0.176841       0.189215   \n",
       "...              ...            ...            ...            ...   \n",
       "3464       -0.058843      -0.003134      -0.397526       0.589346   \n",
       "9411       -0.023053      -0.002466      -0.444291       0.633426   \n",
       "9519       -0.022273      -0.037960      -0.374827       0.669957   \n",
       "10011      -0.011131      -0.038508      -0.428401       0.821097   \n",
       "4678        0.235931       0.133260      -0.193747       0.769481   \n",
       "\n",
       "       vector_dim_100  \n",
       "9454        -0.633315  \n",
       "539         -0.366795  \n",
       "7825        -0.339501  \n",
       "3874        -0.361757  \n",
       "2351        -0.138940  \n",
       "...               ...  \n",
       "3464        -0.214470  \n",
       "9411        -0.217753  \n",
       "9519        -0.326614  \n",
       "10011       -0.333147  \n",
       "4678        -0.100121  \n",
       "\n",
       "[6658 rows x 107 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "undersampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "spam    3329\n",
       "ham     3329\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "undersampled_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=undersampled_df.drop(columns='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>num_stop_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_punctuation_chars</th>\n",
       "      <th>word_count_after_preprocessing</th>\n",
       "      <th>num_chars_after_preprocessing</th>\n",
       "      <th>vector_dim_1</th>\n",
       "      <th>vector_dim_2</th>\n",
       "      <th>vector_dim_3</th>\n",
       "      <th>vector_dim_4</th>\n",
       "      <th>...</th>\n",
       "      <th>vector_dim_91</th>\n",
       "      <th>vector_dim_92</th>\n",
       "      <th>vector_dim_93</th>\n",
       "      <th>vector_dim_94</th>\n",
       "      <th>vector_dim_95</th>\n",
       "      <th>vector_dim_96</th>\n",
       "      <th>vector_dim_97</th>\n",
       "      <th>vector_dim_98</th>\n",
       "      <th>vector_dim_99</th>\n",
       "      <th>vector_dim_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9454</th>\n",
       "      <td>478</td>\n",
       "      <td>111</td>\n",
       "      <td>2329</td>\n",
       "      <td>94</td>\n",
       "      <td>225</td>\n",
       "      <td>1556</td>\n",
       "      <td>-0.778515</td>\n",
       "      <td>-0.034710</td>\n",
       "      <td>-1.238543</td>\n",
       "      <td>0.211485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220476</td>\n",
       "      <td>0.550449</td>\n",
       "      <td>-0.196538</td>\n",
       "      <td>0.359155</td>\n",
       "      <td>0.893357</td>\n",
       "      <td>0.095854</td>\n",
       "      <td>0.149918</td>\n",
       "      <td>-0.342361</td>\n",
       "      <td>0.575483</td>\n",
       "      <td>-0.633315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.765793</td>\n",
       "      <td>-0.064920</td>\n",
       "      <td>-1.341933</td>\n",
       "      <td>-0.418890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519094</td>\n",
       "      <td>0.500147</td>\n",
       "      <td>0.022529</td>\n",
       "      <td>0.176661</td>\n",
       "      <td>0.185421</td>\n",
       "      <td>-0.198955</td>\n",
       "      <td>0.122405</td>\n",
       "      <td>-0.329255</td>\n",
       "      <td>0.883244</td>\n",
       "      <td>-0.366795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7825</th>\n",
       "      <td>94</td>\n",
       "      <td>11</td>\n",
       "      <td>424</td>\n",
       "      <td>27</td>\n",
       "      <td>50</td>\n",
       "      <td>302</td>\n",
       "      <td>-0.373608</td>\n",
       "      <td>-0.115300</td>\n",
       "      <td>-1.290435</td>\n",
       "      <td>-0.243554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518768</td>\n",
       "      <td>0.173426</td>\n",
       "      <td>0.061016</td>\n",
       "      <td>0.393073</td>\n",
       "      <td>0.168801</td>\n",
       "      <td>-0.035846</td>\n",
       "      <td>-0.023936</td>\n",
       "      <td>-0.273413</td>\n",
       "      <td>0.810894</td>\n",
       "      <td>-0.339501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>92</td>\n",
       "      <td>-0.496377</td>\n",
       "      <td>-0.416542</td>\n",
       "      <td>-1.436387</td>\n",
       "      <td>-0.110053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330355</td>\n",
       "      <td>0.496844</td>\n",
       "      <td>-0.158790</td>\n",
       "      <td>0.455779</td>\n",
       "      <td>0.295092</td>\n",
       "      <td>-0.023337</td>\n",
       "      <td>0.315523</td>\n",
       "      <td>-0.158625</td>\n",
       "      <td>0.811924</td>\n",
       "      <td>-0.361757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.257812</td>\n",
       "      <td>-0.004569</td>\n",
       "      <td>-0.342380</td>\n",
       "      <td>-0.104635</td>\n",
       "      <td>...</td>\n",
       "      <td>0.222268</td>\n",
       "      <td>0.197743</td>\n",
       "      <td>-0.013578</td>\n",
       "      <td>0.007772</td>\n",
       "      <td>0.123522</td>\n",
       "      <td>-0.029257</td>\n",
       "      <td>-0.026652</td>\n",
       "      <td>-0.176841</td>\n",
       "      <td>0.189215</td>\n",
       "      <td>-0.138940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>177</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>124</td>\n",
       "      <td>-0.355055</td>\n",
       "      <td>-0.042373</td>\n",
       "      <td>-0.934360</td>\n",
       "      <td>-0.180555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377327</td>\n",
       "      <td>0.277465</td>\n",
       "      <td>0.046666</td>\n",
       "      <td>0.097335</td>\n",
       "      <td>0.166501</td>\n",
       "      <td>-0.058843</td>\n",
       "      <td>-0.003134</td>\n",
       "      <td>-0.397526</td>\n",
       "      <td>0.589346</td>\n",
       "      <td>-0.214470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9411</th>\n",
       "      <td>43</td>\n",
       "      <td>12</td>\n",
       "      <td>218</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>145</td>\n",
       "      <td>-0.397201</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>-0.906342</td>\n",
       "      <td>-0.262234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415713</td>\n",
       "      <td>0.105170</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.094557</td>\n",
       "      <td>0.160885</td>\n",
       "      <td>-0.023053</td>\n",
       "      <td>-0.002466</td>\n",
       "      <td>-0.444291</td>\n",
       "      <td>0.633426</td>\n",
       "      <td>-0.217753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9519</th>\n",
       "      <td>103</td>\n",
       "      <td>22</td>\n",
       "      <td>557</td>\n",
       "      <td>24</td>\n",
       "      <td>54</td>\n",
       "      <td>397</td>\n",
       "      <td>-0.430365</td>\n",
       "      <td>-0.035594</td>\n",
       "      <td>-0.832599</td>\n",
       "      <td>-0.171853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540451</td>\n",
       "      <td>0.257317</td>\n",
       "      <td>-0.164706</td>\n",
       "      <td>0.015197</td>\n",
       "      <td>0.215511</td>\n",
       "      <td>-0.022273</td>\n",
       "      <td>-0.037960</td>\n",
       "      <td>-0.374827</td>\n",
       "      <td>0.669957</td>\n",
       "      <td>-0.326614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>101</td>\n",
       "      <td>30</td>\n",
       "      <td>536</td>\n",
       "      <td>19</td>\n",
       "      <td>52</td>\n",
       "      <td>372</td>\n",
       "      <td>-0.328155</td>\n",
       "      <td>-0.081961</td>\n",
       "      <td>-1.136878</td>\n",
       "      <td>-0.347928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568055</td>\n",
       "      <td>0.193118</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>0.192594</td>\n",
       "      <td>0.087289</td>\n",
       "      <td>-0.011131</td>\n",
       "      <td>-0.038508</td>\n",
       "      <td>-0.428401</td>\n",
       "      <td>0.821097</td>\n",
       "      <td>-0.333147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4678</th>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.201455</td>\n",
       "      <td>-0.253204</td>\n",
       "      <td>-1.155382</td>\n",
       "      <td>-0.368807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461047</td>\n",
       "      <td>-0.263939</td>\n",
       "      <td>-0.138482</td>\n",
       "      <td>0.153497</td>\n",
       "      <td>0.241605</td>\n",
       "      <td>0.235931</td>\n",
       "      <td>0.133260</td>\n",
       "      <td>-0.193747</td>\n",
       "      <td>0.769481</td>\n",
       "      <td>-0.100121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6658 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_count  num_stop_words  num_chars  num_punctuation_chars  \\\n",
       "9454          478             111       2329                     94   \n",
       "539             3               0         22                      4   \n",
       "7825           94              11        424                     27   \n",
       "3874           36              16        147                      2   \n",
       "2351            5               0         25                      2   \n",
       "...           ...             ...        ...                    ...   \n",
       "3464           27               4        177                      7   \n",
       "9411           43              12        218                      7   \n",
       "9519          103              22        557                     24   \n",
       "10011         101              30        536                     19   \n",
       "4678           12               7         44                      0   \n",
       "\n",
       "       word_count_after_preprocessing  num_chars_after_preprocessing  \\\n",
       "9454                              225                           1556   \n",
       "539                                 3                             13   \n",
       "7825                               50                            302   \n",
       "3874                               20                             92   \n",
       "2351                                5                             23   \n",
       "...                               ...                            ...   \n",
       "3464                               18                            124   \n",
       "9411                               22                            145   \n",
       "9519                               54                            397   \n",
       "10011                              52                            372   \n",
       "4678                                5                             24   \n",
       "\n",
       "       vector_dim_1  vector_dim_2  vector_dim_3  vector_dim_4  ...  \\\n",
       "9454      -0.778515     -0.034710     -1.238543      0.211485  ...   \n",
       "539       -0.765793     -0.064920     -1.341933     -0.418890  ...   \n",
       "7825      -0.373608     -0.115300     -1.290435     -0.243554  ...   \n",
       "3874      -0.496377     -0.416542     -1.436387     -0.110053  ...   \n",
       "2351      -0.257812     -0.004569     -0.342380     -0.104635  ...   \n",
       "...             ...           ...           ...           ...  ...   \n",
       "3464      -0.355055     -0.042373     -0.934360     -0.180555  ...   \n",
       "9411      -0.397201      0.006813     -0.906342     -0.262234  ...   \n",
       "9519      -0.430365     -0.035594     -0.832599     -0.171853  ...   \n",
       "10011     -0.328155     -0.081961     -1.136878     -0.347928  ...   \n",
       "4678      -0.201455     -0.253204     -1.155382     -0.368807  ...   \n",
       "\n",
       "       vector_dim_91  vector_dim_92  vector_dim_93  vector_dim_94  \\\n",
       "9454        0.220476       0.550449      -0.196538       0.359155   \n",
       "539         0.519094       0.500147       0.022529       0.176661   \n",
       "7825        0.518768       0.173426       0.061016       0.393073   \n",
       "3874        0.330355       0.496844      -0.158790       0.455779   \n",
       "2351        0.222268       0.197743      -0.013578       0.007772   \n",
       "...              ...            ...            ...            ...   \n",
       "3464        0.377327       0.277465       0.046666       0.097335   \n",
       "9411        0.415713       0.105170       0.004165       0.094557   \n",
       "9519        0.540451       0.257317      -0.164706       0.015197   \n",
       "10011       0.568055       0.193118      -0.091383       0.192594   \n",
       "4678        0.461047      -0.263939      -0.138482       0.153497   \n",
       "\n",
       "       vector_dim_95  vector_dim_96  vector_dim_97  vector_dim_98  \\\n",
       "9454        0.893357       0.095854       0.149918      -0.342361   \n",
       "539         0.185421      -0.198955       0.122405      -0.329255   \n",
       "7825        0.168801      -0.035846      -0.023936      -0.273413   \n",
       "3874        0.295092      -0.023337       0.315523      -0.158625   \n",
       "2351        0.123522      -0.029257      -0.026652      -0.176841   \n",
       "...              ...            ...            ...            ...   \n",
       "3464        0.166501      -0.058843      -0.003134      -0.397526   \n",
       "9411        0.160885      -0.023053      -0.002466      -0.444291   \n",
       "9519        0.215511      -0.022273      -0.037960      -0.374827   \n",
       "10011       0.087289      -0.011131      -0.038508      -0.428401   \n",
       "4678        0.241605       0.235931       0.133260      -0.193747   \n",
       "\n",
       "       vector_dim_99  vector_dim_100  \n",
       "9454        0.575483       -0.633315  \n",
       "539         0.883244       -0.366795  \n",
       "7825        0.810894       -0.339501  \n",
       "3874        0.811924       -0.361757  \n",
       "2351        0.189215       -0.138940  \n",
       "...              ...             ...  \n",
       "3464        0.589346       -0.214470  \n",
       "9411        0.633426       -0.217753  \n",
       "9519        0.669957       -0.326614  \n",
       "10011       0.821097       -0.333147  \n",
       "4678        0.769481       -0.100121  \n",
       "\n",
       "[6658 rows x 106 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=undersampled_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9454     spam\n",
       "539       ham\n",
       "7825     spam\n",
       "3874      ham\n",
       "2351      ham\n",
       "         ... \n",
       "3464     spam\n",
       "9411     spam\n",
       "9519     spam\n",
       "10011    spam\n",
       "4678      ham\n",
       "Name: label, Length: 6658, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=500, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=500, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=500, solver='liblinear')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=500,solver='liblinear')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the scaled test set\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[571  88]\n",
      " [ 73 600]]\n"
     ]
    }
   ],
   "source": [
    "# Print confusion matrix and classification report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.89      0.87      0.88       659\n",
      "        spam       0.87      0.89      0.88       673\n",
      "\n",
      "    accuracy                           0.88      1332\n",
      "   macro avg       0.88      0.88      0.88      1332\n",
      "weighted avg       0.88      0.88      0.88      1332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4644887228386474"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "roc_auc_score(y_test, y_pred_proba[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5355045128938213"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "roc_auc_score(y_test, y_pred_proba[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling is a technique used to balance imbalanced datasets. When a dataset has \n",
    "unequal class distribution, meaning one class (the majority class) has more samples and \n",
    "another class (the minority class) has fewer samples, oversampling is used to increase the size \n",
    "of the minority class to create a balance.\n",
    "\n",
    "\n",
    "The concept of oversampling is to increase the number of examples in the minority class to \n",
    "bring it closer to the majority class in terms of size. The objective is to provide balanced \n",
    "training data to machine learning algorithms so that they can learn patterns from both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('training_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14617, 107)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7959"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into majority and minority classes\n",
    "majority_class = df[df['label'] == 'ham']\n",
    "minority_class = df[df['label'] == 'spam']\n",
    "\n",
    "# Determine the number of samples to add to the minority class\n",
    "samples_needed = len(majority_class) - len(minority_class)\n",
    "\n",
    "samples_needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['label'])\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({'ham': 11288, 'spam': 3329})\n",
      "New class distribution after oversampling: Counter({'ham': 11288, 'spam': 11288})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "# Check the original class distribution\n",
    "original_class_distribution = Counter(y)\n",
    "print(\"Original class distribution:\", original_class_distribution)\n",
    "\n",
    "# Initialize RandomOverSampler\n",
    "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Apply random oversampling to balance the dataset\n",
    "X_oversampled, y_oversampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "# Check the new class distribution after oversampling\n",
    "new_class_distribution = Counter(y_oversampled)\n",
    "print(\"New class distribution after oversampling:\", new_class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_oversampled, y_oversampled, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=500, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=500, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=500, solver='liblinear')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=500,solver='liblinear')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the scaled test set\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[2943  435]\n",
      " [ 330 3065]]\n"
     ]
    }
   ],
   "source": [
    "# Print confusion matrix and classification report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.90      0.87      0.88      3378\n",
      "        spam       0.88      0.90      0.89      3395\n",
      "\n",
      "    accuracy                           0.89      6773\n",
      "   macro avg       0.89      0.89      0.89      6773\n",
      "weighted avg       0.89      0.89      0.89      6773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5875867499221769"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "roc_auc_score(y_test, y_pred_proba[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4136260704497873"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "roc_auc_score(y_test, y_pred_proba[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "\n",
    "---\n",
    "The main concept of SMOTE is to create synthetic examples among existing examples in the\n",
    "minority class. This means generating new samples artificially to balance the data, providing\n",
    "sufficient training data for the model to learn the patterns in the minority class effectively.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({'ham': 11288, 'spam': 3329})\n",
      "New class distribution after SMOTE: Counter({'ham': 11288, 'spam': 11288})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# Check the original class distribution\n",
    "original_class_distribution = Counter(y)\n",
    "print(\"Original class distribution:\", original_class_distribution)\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)  # Auto adjusts to balance the dataset\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "# Check the new class distribution after SMOTE\n",
    "new_class_distribution = Counter(y_smote)\n",
    "print(\"New class distribution after SMOTE:\", new_class_distribution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=500, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=500, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=500, solver='liblinear')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=500,solver='liblinear',penalty='l2')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the scaled test set\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[2982  396]\n",
      " [ 281 3114]]\n"
     ]
    }
   ],
   "source": [
    "# Print confusion matrix and classification report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.91      0.88      0.90      3378\n",
      "        spam       0.89      0.92      0.90      3395\n",
      "\n",
      "    accuracy                           0.90      6773\n",
      "   macro avg       0.90      0.90      0.90      6773\n",
      "weighted avg       0.90      0.90      0.90      6773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6244982913785903"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "roc_auc_score(y_test, y_pred_proba[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37107960981173344"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "roc_auc_score(y_test, y_pred_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open('logistic_regression__best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
